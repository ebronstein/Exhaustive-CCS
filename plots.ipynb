{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "## TODO\n",
    "\n",
    "- Adapt transfer accuracy heatmaps if the method concatenated multiple labeled and/or unlabeled datasets.\n",
    "- Handle case where train prefix != test prefix.\n",
    "\n",
    "## Adding a new method\n",
    "\n",
    "- Update `METHOD_ORDER`.\n",
    "- Update `method_uses_labeled_and_unlabeled_data` if needed. If the method uses labeled and unlabeled data, it should run eval on both the labeled and unlabeled datasets. In this case, we might want to filter `train == test` (train is the unlabeled dataset and test is the eval dataset)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import json\n",
    "import os\n",
    "import pprint\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "from typing import Literal, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "from IPython.display import display\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from utils import plotting as plot_utils\n",
    "from utils.file_utils import get_model_short_name\n",
    "from utils_extraction import metrics\n",
    "from utils_generation import hf_utils\n",
    "from utils_generation.hf_auth_token import HF_AUTH_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# increase figure size to (8, 8)\n",
    "rcParams[\"figure.figsize\"] = (8, 8)\n",
    "rcParams[\"figure.dpi\"] = 150\n",
    "plt.style.use(\"ggplot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_DIR = Path(\"extraction_results\")\n",
    "\n",
    "\n",
    "DEBERTA = \"deberta-v2-xxlarge-mnli\"\n",
    "LLAMA_7B_CHAT = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "DATASET_ORDER = (\n",
    "    \"imdb\",\n",
    "    \"amazon-polarity\",\n",
    "    \"ag-news\",\n",
    "    \"dbpedia-14\",\n",
    "    \"copa\",\n",
    "    \"rte\",\n",
    "    \"boolq\",\n",
    "    \"qnli\",\n",
    "    \"piqa\",\n",
    ")\n",
    "METHOD_ORDER = (\n",
    "    \"CCS\",\n",
    "    \"LR\",\n",
    "    \"Random\",\n",
    "    \"CCS+LR\",\n",
    "    \"CCS-in-LR-span\",\n",
    "    \"CCS+LR-in-span\",\n",
    "    \"CCS-md\",\n",
    "    \"LR-md\",\n",
    "    \"Random-md\",\n",
    "    \"pseudolabel\",\n",
    ")\n",
    "\n",
    "# Methods that combine CCS and LR.\n",
    "CCS_LR_METHODS = (\"CCS+LR\", \"CCS+LR-in-span\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading/preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading and preprocessing\n",
    "\n",
    "\n",
    "def reduce_ccs_lr_method(df):\n",
    "    method = df[\"method\"].unique()\n",
    "    if not (len(method) == 1 and method[0] in CCS_LR_METHODS):\n",
    "        raise ValueError(f\"Expected a CCS+LR method, got {method}\")\n",
    "\n",
    "    sup_weight = df[\"sup_weight\"].unique()\n",
    "    if len(sup_weight) != 1:\n",
    "        raise ValueError(f\"Expected one unique sup_weight, got {sup_weight}\")\n",
    "    sup_weight = sup_weight[0]\n",
    "\n",
    "    unsup_weight = df[\"unsup_weight\"].unique()\n",
    "    if len(unsup_weight) != 1:\n",
    "        raise ValueError(f\"Expected one unique unsup_weight, got {unsup_weight}\")\n",
    "    unsup_weight = unsup_weight[0]\n",
    "\n",
    "    if sup_weight != 0 and unsup_weight != 0:\n",
    "        return \"CCS+LR\"\n",
    "    elif sup_weight != 0 and unsup_weight == 0:\n",
    "        return \"LR\"\n",
    "    elif sup_weight == 0 and unsup_weight != 0:\n",
    "        return \"CCS\"\n",
    "    else:\n",
    "        raise ValueError(\"sup_weight and unsup_weight should not both be 0.\")\n",
    "\n",
    "\n",
    "def method_uses_labeled_and_unlabeled_data(method_df: pd.DataFrame) -> bool:\n",
    "    method = method_df[\"method\"].unique()\n",
    "    if len(method) != 1:\n",
    "        raise ValueError(f\"Expected one unique method, got {method}\")\n",
    "    method = method[0]\n",
    "\n",
    "    if method == \"CCS+LR\":\n",
    "        method = reduce_ccs_lr_method(method_df)\n",
    "        if method == \"CCS+LR\":\n",
    "            return True\n",
    "        elif method == \"CCS\" or method == \"LR\":\n",
    "            return False\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Unexpected method returned from reduce_ccs_lr_method: {method}\"\n",
    "            )\n",
    "\n",
    "    return method in (\n",
    "        \"CCS-in-LR-span\",\n",
    "        \"CCS+LR-in-span\",\n",
    "        \"CCS-select-LR\",\n",
    "        \"pseudolabel\",\n",
    "    )\n",
    "\n",
    "\n",
    "def filter_train_equals_test_per_method(df, is_ccs_lr_in_span_oracle: bool = False):\n",
    "    \"\"\"Filters rows where train == test if the method requires it.\n",
    "\n",
    "    Condition to filter train == test: the method uses labeled and unlabeled\n",
    "    data by training on the unlabeled data on which it is tested.\n",
    "\n",
    "    Example use case: when plotting transfer accuracy of methods that use\n",
    "    labeled and unlabeled data, we should evaluate the transfer accuracy by\n",
    "    using the test set as the unlabeled train set, which corresponds to the\n",
    "    \"train\" column for such methods.\n",
    "    \"\"\"\n",
    "    method_dfs = []\n",
    "    for method, method_df in df.groupby(\"method\"):\n",
    "        if method_uses_labeled_and_unlabeled_data(method_df) and not (\n",
    "            is_ccs_lr_in_span_oracle and method == \"CCS+LR-in-span\"\n",
    "        ):\n",
    "            method_df = method_df[method_df[\"train\"] == method_df[\"test\"]]\n",
    "\n",
    "        method_dfs.append(method_df)\n",
    "\n",
    "    return pd.concat(method_dfs, ignore_index=True)\n",
    "\n",
    "\n",
    "def check_for_combined_datasets(datasets_str: str) -> bool:\n",
    "    pattern = r\"nolabel_[\\w-]+-label_[\\w-]+\"\n",
    "    return re.match(pattern, datasets_str) is not None\n",
    "\n",
    "\n",
    "def parse_combined_datasets(combined_datasets_str):\n",
    "    if not check_for_combined_datasets(combined_datasets_str):\n",
    "        return combined_datasets_str, \"\"\n",
    "\n",
    "    no_label_train, label_train = combined_datasets_str.split(\"-label_\")\n",
    "    if not no_label_train.startswith(\"nolabel_\"):\n",
    "        raise ValueError(\n",
    "            f\"Expected no_label_train to start with 'no_label_', got {no_label_train}\"\n",
    "        )\n",
    "\n",
    "    train = no_label_train[len(\"nolabel_\") :]\n",
    "    train_labeled = label_train\n",
    "    return train, train_labeled\n",
    "\n",
    "\n",
    "def get_labeled_and_unlabeled_train_data(\n",
    "    row: pd.Series,\n",
    ") -> tuple[str, Optional[str]]:\n",
    "    \"\"\"Get the main train dataset and the labeled train dataset names.\n",
    "\n",
    "    Args:\n",
    "        row: Experiment results DataFrame row.\n",
    "\n",
    "    Returns:\n",
    "        A tuple of the main train dataset names and the labeled train dataset\n",
    "        names. If the method uses only labeled datasets (e.g., LR) or only\n",
    "        unlabeled datasets (e.g., CCS), these datasets are considered the main\n",
    "        datasets, and the second element of the tuple is an empty string.\n",
    "        Otherwise, the first element is the unlabeled dataset and the second\n",
    "        element is the labeled dataset (e.g., for CCS+LR).\n",
    "    \"\"\"\n",
    "    return list(parse_combined_datasets(row.train))\n",
    "\n",
    "\n",
    "def flatten_dictionary(d, parent_key=\"\", sep=\".\"):\n",
    "    \"\"\"\n",
    "    Flatten a nested dictionary.\n",
    "\n",
    "    Args:\n",
    "        d (dict): The nested dictionary to flatten.\n",
    "        parent_key (str): The parent key (used in recursion).\n",
    "        sep (str): The separator for joining keys.\n",
    "\n",
    "    Returns:\n",
    "        dict: Flattened dictionary.\n",
    "    \"\"\"\n",
    "    items = []\n",
    "    for k, v in d.items():\n",
    "        new_key = f\"{parent_key}{sep}{k}\" if parent_key else k\n",
    "        if isinstance(v, dict):\n",
    "            items.extend(flatten_dictionary(v, parent_key=new_key, sep=sep).items())\n",
    "        else:\n",
    "            items.append((new_key, v))\n",
    "    return dict(items)\n",
    "\n",
    "\n",
    "def get_orthogonal_directions_dataset(load_orthogonal_directions_dir: str) -> str:\n",
    "    \"\"\"Return the dataset for which the orthogonal directions were computed.\"\"\"\n",
    "    combined_datasets_str = os.path.basename(\n",
    "        os.path.normpath(load_orthogonal_directions_dir)\n",
    "    )\n",
    "    return parse_combined_datasets(combined_datasets_str)[1]\n",
    "\n",
    "\n",
    "def reformat_oracle_lr_in_span_df(df):\n",
    "    \"\"\"Reformat the train and train_labeled columns for the oracle LR in span method.\"\"\"\n",
    "    rest_df = df[df[\"method\"] != \"CCS+LR-in-span\"].copy()\n",
    "    df = df[df[\"method\"] == \"CCS+LR-in-span\"].copy()\n",
    "    df = df[df[\"train\"] == df[\"train_labeled\"]]\n",
    "    df = df[df[\"train\"] == df[\"test\"]]\n",
    "    df = df.copy()\n",
    "    df[\"train\"] = df.load_orthogonal_directions_dir.apply(\n",
    "        get_orthogonal_directions_dataset\n",
    "    )\n",
    "    df[\"train_labeled\"] = df[\"train\"]\n",
    "\n",
    "    df = pd.concat([df, rest_df], ignore_index=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_eval_csvs(\n",
    "    base_path,\n",
    "    pattern: Optional[str] = None,\n",
    "    model_filter=None,\n",
    "    train_set_filter=None,\n",
    "    eval_set_filter=None,\n",
    "    use_latest_run_id: bool = True,\n",
    "    use_best_in_domain_acc: bool = True,\n",
    "    is_ccs_lr_in_span_oracle: bool = False,\n",
    "    verbose: bool = True,\n",
    ") -> dict[str, pd.DataFrame]:\n",
    "    base_path = Path(base_path)\n",
    "    if pattern:\n",
    "        if not pattern.endswith(\"eval.csv\"):\n",
    "            pattern = os.path.join(pattern, \"**/eval.csv\")\n",
    "    else:\n",
    "        pattern = \"**/eval.csv\"\n",
    "    print(\"pattern:\", pattern)\n",
    "    csv_files = base_path.glob(pattern)\n",
    "    exp_name_to_dfs = defaultdict(list)\n",
    "\n",
    "    for csv_file in tqdm(list(csv_files)):\n",
    "        # Extract components from the path\n",
    "        parts = csv_file.parts\n",
    "        experiment_name = os.path.join(*parts[1:-7])\n",
    "        train_set = parts[-6]\n",
    "        seed = parts[-5].split(\"_\")[-1]\n",
    "        run_id = parts[-4]\n",
    "        eval_dataset = parts[-2]\n",
    "\n",
    "        # Read the CSV\n",
    "        df = pd.read_csv(csv_file)\n",
    "\n",
    "        # Sanity checks\n",
    "        if not all(df[\"train\"] == train_set) or not all(df[\"test\"] == eval_dataset):\n",
    "            raise ValueError(f\"Sanity check failed for {csv_file}\")\n",
    "\n",
    "        # Set \"train\" and \"train_labeled\" columns based on the \"train\" dataset\n",
    "        # string, which may combine labeled and unlabeled datasets (e.g., for\n",
    "        # CCS+LR method).\n",
    "        train_and_train_labeled_df = pd.DataFrame(\n",
    "            list(df.apply(get_labeled_and_unlabeled_train_data, axis=1)),\n",
    "            columns=[\"train\", \"train_labeled\"],\n",
    "        )\n",
    "        df[\"train\"] = train_and_train_labeled_df[\"train\"]\n",
    "        train_col_loc = df.columns.get_loc(\"train\")\n",
    "        df.insert(\n",
    "            train_col_loc + 1,\n",
    "            \"train_labeled\",\n",
    "            train_and_train_labeled_df[\"train_labeled\"],\n",
    "        )\n",
    "\n",
    "        # Add new columns\n",
    "        df[\"experiment_name\"] = experiment_name\n",
    "        df[\"seed\"] = int(seed)\n",
    "        df[\"run_id\"] = int(run_id)\n",
    "\n",
    "        # Add columns from config.\n",
    "        eval_dir = os.path.join(*csv_file.parts[:-3])\n",
    "        config_path = Path(eval_dir) / \"config.json\"\n",
    "        if config_path.exists():\n",
    "            with open(config_path, \"r\") as f:\n",
    "                config = json.load(f)\n",
    "\n",
    "            # Remove type annotations.\n",
    "            del config[\"__annotations__\"]\n",
    "            config = flatten_dictionary(config)\n",
    "\n",
    "            for key, value in config.items():\n",
    "                # If the value is an iterable, repeat it for each row.\n",
    "                # Otherwise, pandas assumes len(value) == len(df).\n",
    "                if isinstance(value, (list, tuple)):\n",
    "                    value = [value] * len(df)\n",
    "                df[key] = value\n",
    "\n",
    "        # Reformat the train and train_labeled columns for the oracle LR in span method.\n",
    "        if \"CCS+LR-in-span\" in df.method.unique() and is_ccs_lr_in_span_oracle:\n",
    "            df = reformat_oracle_lr_in_span_df(df)\n",
    "\n",
    "        exp_name_to_dfs[experiment_name].append(df)\n",
    "\n",
    "    if not exp_name_to_dfs:\n",
    "        raise ValueError(f\"No eval CSV files found in {base_path}.\")\n",
    "\n",
    "    exp_name_to_final_df = {}\n",
    "    for exp_name, dfs in exp_name_to_dfs.items():\n",
    "        # Concatenate all dataframes\n",
    "        exp_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "        # Optional filtering\n",
    "        if model_filter:\n",
    "            exp_df = exp_df[exp_df[\"model\"] == model_filter]\n",
    "        if train_set_filter:\n",
    "            exp_df = exp_df[exp_df[\"train\"] == train_set_filter]\n",
    "        if eval_set_filter:\n",
    "            exp_df = exp_df[exp_df[\"test\"] == eval_set_filter]\n",
    "\n",
    "        if use_latest_run_id:\n",
    "            exp_df = get_latest_run_id_df(exp_df)\n",
    "        if use_best_in_domain_acc:\n",
    "            exp_df = get_best_metric_by_acc_df(exp_df)\n",
    "\n",
    "        exp_name_to_final_df[exp_name] = exp_df\n",
    "\n",
    "    return exp_name_to_final_df\n",
    "\n",
    "\n",
    "def get_latest_run_id_df(df):\n",
    "    latest_group_dfs = []\n",
    "    for key, group_df in df.groupby(\n",
    "        [\n",
    "            \"model\",\n",
    "            \"prefix\",\n",
    "            \"method\",\n",
    "            \"mode\",\n",
    "            \"train\",\n",
    "            \"train_labeled\",\n",
    "            \"test\",\n",
    "            \"seed\",\n",
    "            \"num_orthogonal_directions\",\n",
    "            \"load_orthogonal_directions_dir\",\n",
    "        ],\n",
    "        dropna=False,\n",
    "    ):\n",
    "        max_run_id = group_df.run_id.max()\n",
    "        group_df = group_df[group_df.run_id == max_run_id]\n",
    "        latest_group_dfs.append(group_df)\n",
    "\n",
    "    latest_run_df = pd.concat(latest_group_dfs, ignore_index=True)\n",
    "\n",
    "    for key, group_df in latest_run_df.groupby(\n",
    "        [\n",
    "            \"model\",\n",
    "            \"prefix\",\n",
    "            \"method\",\n",
    "            \"mode\",\n",
    "            \"train\",\n",
    "            \"train_labeled\",\n",
    "            \"test\",\n",
    "            \"prompt_level\",\n",
    "            \"location\",\n",
    "            \"layer\",\n",
    "            \"seed\",\n",
    "            \"num_orthogonal_directions\",\n",
    "            \"load_orthogonal_directions_dir\",\n",
    "        ],\n",
    "        dropna=False,\n",
    "    ):\n",
    "        num_unique_run_ids = group_df.run_id.nunique()\n",
    "        if num_unique_run_ids != 1:\n",
    "            raise ValueError(\n",
    "                f\"Expected 1 unique run_id, but got {num_unique_run_ids} for {key}\"\n",
    "            )\n",
    "\n",
    "    return latest_run_df\n",
    "\n",
    "\n",
    "def get_dataset_names_from_str(train_sets_str: str) -> list[str]:\n",
    "    return train_sets_str.split(\"+\")\n",
    "\n",
    "\n",
    "def get_best_metric_by_acc_df(df):\n",
    "    \"\"\"Flip metrics for each experiment if it improves in-domain accuracy.\n",
    "\n",
    "    Use the in-domain accuracy to decide whether to flip the accuracy instead of\n",
    "    computing max(acc, 1-acc) for test datasets even if the in-domain accuracy\n",
    "    is greater than 0.5. Also flips the ECE if the flipped accuracy is better.\n",
    "\n",
    "    In-domain accuracy for a given dataset is computed as the mean accuracy over\n",
    "    all the prompts when evaluated on that same dataset. The model, prefix,\n",
    "    method, token location, layer, and seed are held constant.\n",
    "    \"\"\"\n",
    "    processed_df = []\n",
    "    for key, train_df in df.groupby(\n",
    "        [\n",
    "            \"model\",\n",
    "            \"prefix\",\n",
    "            \"method\",\n",
    "            \"mode\",\n",
    "            \"train\",\n",
    "            \"train_labeled\",\n",
    "            \"location\",\n",
    "            \"layer\",\n",
    "            \"seed\",\n",
    "            \"num_orthogonal_directions\",\n",
    "        ],\n",
    "        dropna=False,\n",
    "    ):\n",
    "        assert len(train_df[\"train\"].unique()) == 1\n",
    "        train_ds_str = train_df[\"train\"].iloc[0]\n",
    "        train_ds_list = get_dataset_names_from_str(train_ds_str)\n",
    "\n",
    "        assert len(train_df[\"train_labeled\"].unique()) == 1\n",
    "        train_labeled_ds_str = train_df[\"train_labeled\"].iloc[0]\n",
    "        train_labeled_ds_list = (\n",
    "            get_dataset_names_from_str(train_labeled_ds_str)\n",
    "            if train_labeled_ds_str is not None\n",
    "            else []\n",
    "        )\n",
    "\n",
    "        # If the method uses both labeled and unlabeled datasets, only use the\n",
    "        # labeled datasets as the \"in-domain\" datasets to decide whether to flip\n",
    "        # the accuracy.\n",
    "        in_domain_train_ds_list = (\n",
    "            train_labeled_ds_list if train_labeled_ds_str else train_ds_list\n",
    "        )\n",
    "        if not in_domain_train_ds_list:\n",
    "            raise ValueError(f\"No train datasets found for {key}\")\n",
    "\n",
    "        # If trained on multiple datasets, use the mean accuracy on\n",
    "        # those datasets to decide whether to flip the accuracy. Otherwise,\n",
    "        # use the accuracy of the single dataset.\n",
    "        in_domain_train_df = train_df[train_df[\"test\"].isin(in_domain_train_ds_list)]\n",
    "        if len(in_domain_train_df) == 0:\n",
    "            raise ValueError(\n",
    "                f\"No in-domain train datasets found for {key} with train \"\n",
    "                f\"datasets {in_domain_train_ds_list}\"\n",
    "            )\n",
    "        group_num_unique_in_domain_prompts = in_domain_train_df.prompt_level.nunique()\n",
    "        num_unique_in_domain_prompts = df[\n",
    "            df[\"test\"].isin(in_domain_train_ds_list)\n",
    "        ].prompt_level.nunique()\n",
    "        if group_num_unique_in_domain_prompts != num_unique_in_domain_prompts:\n",
    "            raise ValueError(\n",
    "                f\"Expected {num_unique_in_domain_prompts} unique \"\n",
    "                f\"prompts in the group df for key {key} to match the \"\n",
    "                \"number of unique prompt indices in the whole df where \"\n",
    "                f\"test == {train_ds_str}, but got \"\n",
    "                f\"{group_num_unique_in_domain_prompts}.\"\n",
    "            )\n",
    "\n",
    "        in_domain_acc = in_domain_train_df[\"accuracy\"].mean()\n",
    "        if in_domain_acc < 0.5:\n",
    "            train_df[\"accuracy\"] = 1 - train_df[\"accuracy\"]\n",
    "            train_df[\"ece\"] = train_df[\"ece_flip\"]\n",
    "\n",
    "        del train_df[\"ece_flip\"]\n",
    "        processed_df.append(train_df)\n",
    "\n",
    "    return pd.concat(processed_df)\n",
    "\n",
    "\n",
    "# Inference\n",
    "\n",
    "InferenceMethodType = Literal[\"auto\", \"mean\", \"class0\", \"class1\"]\n",
    "\n",
    "\n",
    "def get_single_class_probs(\n",
    "    probs_df: np.ndarray,\n",
    "    method: str,\n",
    "    inference_method: InferenceMethodType = \"auto\",\n",
    "):\n",
    "    \"\"\"Returns the predicted probability for a single class.\"\"\"\n",
    "    if method == \"LR\":\n",
    "        if inference_method not in (\"auto\", \"class1\"):\n",
    "            raise ValueError(\n",
    "                f\"Unsupported inference method {inference_method} for method {method}\"\n",
    "            )\n",
    "        # Older versions of the saved LR probabilities used \"prediction\" as the\n",
    "        # column name, and newer versions use \"prob\".\n",
    "        probs = probs_df.prob if \"prob\" in probs_df else probs_df.prediction\n",
    "    elif method == \"CCS\":\n",
    "        if inference_method == \"auto\" or inference_method == \"mean\":\n",
    "            # Arbitrarily treat p0 as 0 and p1 as 1 to match the original CCS\n",
    "            # implementation, which computes (p0 + 1 - p1) / 2 but then checks\n",
    "            # if this value is < 0.5 intead of > 0.5.\n",
    "            probs = (probs_df.p1 + 1 - probs_df.p0) / 2\n",
    "        elif inference_method == \"class0\":\n",
    "            probs = probs_df.p0\n",
    "        elif inference_method == \"class1\":\n",
    "            probs = probs_df.p1\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown method {method}\")\n",
    "\n",
    "    return probs.values.astype(np.float32)\n",
    "\n",
    "\n",
    "# ECE\n",
    "\n",
    "\n",
    "def make_ece_df(\n",
    "    save_dir: str,\n",
    "    experiment_name: str,\n",
    "):\n",
    "    base_path = Path(save_dir) / experiment_name\n",
    "    probs_files = base_path.glob(\"**/prob*.csv\")\n",
    "\n",
    "    all_data = []\n",
    "    for probs_file in probs_files:\n",
    "        # Extract components from the path\n",
    "        parts = probs_file.parts\n",
    "        model = parts[-7]\n",
    "        train_set = parts[-6]\n",
    "        seed = int(parts[-5].split(\"_\")[-1])\n",
    "        run_id = int(parts[-4])\n",
    "        eval_set = parts[-2]\n",
    "        probs_str, method, prompt = parts[-1].split(\".\")[0].split(\"_\")\n",
    "        prompt = int(prompt)\n",
    "\n",
    "        # Read the CSV\n",
    "        df = pd.read_csv(probs_file)\n",
    "\n",
    "        probs = get_single_class_probs(df, method)\n",
    "        preds = (probs > 0.5).astype(int)\n",
    "        labels = df.label.values\n",
    "        acc = (preds == labels).mean()\n",
    "        ece, bin_mean_probs, bin_mean_labels = metrics.expected_calibration_error(\n",
    "            probs, labels\n",
    "        )\n",
    "        ece_flip, bin_mean_probs_flip, bin_mean_labels_flip = (\n",
    "            metrics.expected_calibration_error(1 - probs, labels)\n",
    "        )\n",
    "        all_data.append(\n",
    "            [\n",
    "                model,\n",
    "                method,\n",
    "                train_set,\n",
    "                eval_set,\n",
    "                prompt,\n",
    "                seed,\n",
    "                run_id,\n",
    "                acc,\n",
    "                ece,\n",
    "                bin_mean_probs,\n",
    "                bin_mean_labels,\n",
    "                False,\n",
    "            ]\n",
    "        )\n",
    "        all_data.append(\n",
    "            [\n",
    "                model,\n",
    "                method,\n",
    "                train_set,\n",
    "                eval_set,\n",
    "                prompt,\n",
    "                seed,\n",
    "                run_id,\n",
    "                1 - acc,\n",
    "                ece_flip,\n",
    "                bin_mean_probs_flip,\n",
    "                bin_mean_labels_flip,\n",
    "                True,\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    ece_df = pd.DataFrame(\n",
    "        all_data,\n",
    "        columns=[\n",
    "            \"model\",\n",
    "            \"method\",\n",
    "            \"train\",\n",
    "            \"test\",\n",
    "            \"prompt\",\n",
    "            \"seed\",\n",
    "            \"run_id\",\n",
    "            \"acc\",\n",
    "            \"ece\",\n",
    "            \"bin_mean_probs\",\n",
    "            \"bin_mean_labels\",\n",
    "            \"flip_probs\",\n",
    "        ],\n",
    "    )\n",
    "    return ece_df\n",
    "\n",
    "\n",
    "def method_uses_orthogonal_directions(method):\n",
    "    return method in (\"CCS+LR\", \"CCS-in-LR-span\", \"CCS-select-LR\")\n",
    "\n",
    "\n",
    "def compute_mean_std_metric_df(df, metric=\"accuracy\"):\n",
    "    \"\"\"Compute the mean and standard deviation of the metric.\"\"\"\n",
    "    return (\n",
    "        df.groupby(\n",
    "            [\n",
    "                \"model\",\n",
    "                \"method\",\n",
    "                \"prefix\",\n",
    "                \"mode\",\n",
    "                \"train\",\n",
    "                \"train_labeled\",\n",
    "                \"test\",\n",
    "                \"location\",\n",
    "                \"layer\",\n",
    "                \"num_orthogonal_directions\",\n",
    "                \"sup_weight\",\n",
    "                \"unsup_weight\",\n",
    "            ],\n",
    "            dropna=False,\n",
    "        )\n",
    "        .agg(mean=(metric, \"mean\"), std=(metric, \"std\"))\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "\n",
    "def make_train_test_df(df: pd.DataFrame, train_col: str) -> pd.DataFrame:\n",
    "    # Modify the pivot table creation to include both mean accuracy and\n",
    "    # std accuracy in the annotations\n",
    "    pivot_table_mean = df.pivot(index=train_col, columns=\"test\", values=\"mean\").reindex(\n",
    "        index=DATASET_ORDER, columns=DATASET_ORDER\n",
    "    )\n",
    "    pivot_table_std = df.pivot(index=train_col, columns=\"test\", values=\"std\").reindex(\n",
    "        index=DATASET_ORDER, columns=DATASET_ORDER\n",
    "    )\n",
    "\n",
    "    # Reorder the pivot tables according to DATASET_ORDER\n",
    "    pivot_table_mean_ordered = pivot_table_mean.reindex(\n",
    "        index=DATASET_ORDER, columns=DATASET_ORDER\n",
    "    )\n",
    "    pivot_table_std_ordered = pivot_table_std.reindex(\n",
    "        index=DATASET_ORDER, columns=DATASET_ORDER\n",
    "    )\n",
    "\n",
    "    return pivot_table_mean_ordered, pivot_table_std_ordered\n",
    "\n",
    "\n",
    "def train_col_name_for_transfer_acc(df: pd.DataFrame) -> str:\n",
    "    method = df[\"method\"].unique()\n",
    "    if len(method) != 1:\n",
    "        raise ValueError(f\"Expected one unique method, got {method}\")\n",
    "    method = method[0]\n",
    "\n",
    "    if method in CCS_LR_METHODS:\n",
    "        method = reduce_ccs_lr_method(df)\n",
    "        if method == \"LR\" or method == \"CCS+LR\":\n",
    "            return \"train_labeled\"\n",
    "        elif method == \"CCS\":\n",
    "            return \"train\"\n",
    "        else:\n",
    "            raise ValueError(f\"Unexpected method {method}\")\n",
    "    elif method in (\"LR\", \"CCS\"):\n",
    "        return \"train\"\n",
    "    elif method == \"pseudolabel\":\n",
    "        return \"train_labeled\"\n",
    "    else:\n",
    "        raise NotImplementedError(f\"Unclear what train column should be for {method}.\")\n",
    "\n",
    "\n",
    "def make_train_test_metric_dfs_dict(df: pd.DataFrame, metric: str) -> dict:\n",
    "    \"\"\"Create train-test metric matrix grouped by separate categories.\n",
    "\n",
    "    Groups the DataFrame by model, method, and num_orthogonal_directions.\n",
    "\n",
    "    Returns:\n",
    "        Nested dictionary where the keys are (model, method, num_orthogonal_directions) and the value is a tuple containing the\n",
    "        mean and standard deviation train-test metric matrix DataFrames.\n",
    "    \"\"\"\n",
    "    df_dict = {}\n",
    "\n",
    "    # Calculate mean accuracy and standard deviation of accuracy for each combination\n",
    "    agg_df = compute_mean_std_metric_df(df, metric=metric)\n",
    "\n",
    "    new_methods = tuple(set(agg_df.method.unique()) - set(METHOD_ORDER))\n",
    "    method_order = METHOD_ORDER + new_methods\n",
    "\n",
    "    for model in agg_df.model.unique():\n",
    "        if model not in df_dict:\n",
    "            df_dict[model] = {}\n",
    "\n",
    "        model_df = agg_df[agg_df[\"model\"] == model]\n",
    "        methods = model_df.method.unique()\n",
    "        # Sort methods by METHOD_ORDER.\n",
    "        methods = sorted(methods, key=lambda x: method_order.index(x))\n",
    "        for method in methods:\n",
    "            if method not in df_dict[model]:\n",
    "                df_dict[model][method] = {}\n",
    "            df = model_df[model_df[\"method\"] == method]\n",
    "            if len(df) == 0:\n",
    "                continue\n",
    "\n",
    "            if method_uses_orthogonal_directions(method):\n",
    "                group_keys_dfs = df.groupby([\"num_orthogonal_directions\"])\n",
    "            else:\n",
    "                group_keys_dfs = [(None, df)]\n",
    "\n",
    "            for num_orthogonal_directions, df in group_keys_dfs:\n",
    "                # These columns should be unique for a given model and method.\n",
    "                # If the method uses labeled and unlabeled data, \"train_labeled\" and\n",
    "                # \"train\" are not unique because train_labeled is the pivot column\n",
    "                # and train == test for each test set since train is the unlabeled\n",
    "                # train set. Otherwise, if the method uses only labeled or only\n",
    "                # unlabeled data, we ignore whether there is a unique\n",
    "                # train or train_labeled for each group because only one of them\n",
    "                # is used for the algorithm.\n",
    "                expected_unique_cols = [\"prefix\", \"mode\", \"location\", \"layer\"]\n",
    "                train_col = train_col_name_for_transfer_acc(df)\n",
    "                # TODO: delete this block.\n",
    "                # if method_uses_labeled_and_unlabeled_data(df):\n",
    "                #     train_col = \"train_labeled\"\n",
    "                # else:\n",
    "                #     # Otherwise, the \"train_labeled\" column should also be unique\n",
    "                #     # because it is empty.\n",
    "                #     train_col = \"train\"\n",
    "                #     expected_unique_cols.append(\"train_labeled\")\n",
    "\n",
    "                for col in expected_unique_cols:\n",
    "                    if df[col].nunique() > 1:\n",
    "                        raise ValueError(\n",
    "                            f\"Expected one unique '{col}' dataset, got \"\n",
    "                            f\"{df[col].unique()} for model={model}, method={method}\"\n",
    "                        )\n",
    "\n",
    "                pivot_table_mean_ordered, pivot_table_std_ordered = make_train_test_df(\n",
    "                    df, train_col\n",
    "                )\n",
    "\n",
    "                df_dict[model][method][num_orthogonal_directions] = (\n",
    "                    pivot_table_mean_ordered,\n",
    "                    pivot_table_std_ordered,\n",
    "                )\n",
    "\n",
    "    return df_dict\n",
    "\n",
    "\n",
    "def compute_mean_train_test_acc_df_over_nonunique_cols(df):\n",
    "    \"\"\"Computes the mean accuracy over non-unique columns in the DataFrame.\n",
    "\n",
    "    Groups by \"train\", \"test\", \"prompt_level\", and any other columns that have\n",
    "    only one unique value. For each group, computes the mean accuracy, and then\n",
    "    concatenates the result.\n",
    "\n",
    "    Example use case: if CCS was run over multiple train_labeled datasets, which\n",
    "    are unused by CCS, take the mean accuracy for each (train, test) pair.\n",
    "    \"\"\"\n",
    "    group_cols = [\"train\", \"test\", \"prompt_level\"]\n",
    "    for col in df.columns:\n",
    "        try:\n",
    "            n_unique = df[col].nunique()\n",
    "            if n_unique == 1:\n",
    "                group_cols.append(col)\n",
    "                # print(col, n_unique)\n",
    "        except TypeError:\n",
    "            # print(col, \"TypeError\")\n",
    "            pass\n",
    "\n",
    "    group_cols = list(set(group_cols))\n",
    "\n",
    "    mean_acc_rows = []\n",
    "    group_cols = [\n",
    "        \"model\",\n",
    "        \"train_prefix\",\n",
    "        \"test_prefix\",\n",
    "        \"method\",\n",
    "        \"prompt_level\",\n",
    "        \"mode\",\n",
    "        \"train\",\n",
    "        \"test\",\n",
    "        \"location\",\n",
    "        \"layer\",\n",
    "    ]\n",
    "    for key, group_df in df.groupby(group_cols):\n",
    "        mean_acc = group_df[\"accuracy\"].mean()\n",
    "        if len(group_df) == 1:\n",
    "            mean_acc_group_df = group_df\n",
    "        else:\n",
    "            mean_acc_group_df = group_df[group_df[\"train\"] == group_df[\"train_labeled\"]]\n",
    "\n",
    "        if len(mean_acc_group_df) != 1:\n",
    "            raise ValueError(\n",
    "                f\"Expected mean_acc_group_df to have 1 row, got {len(mean_acc_group_df)}\"\n",
    "            )\n",
    "\n",
    "        mean_acc_group_df = mean_acc_group_df.copy()\n",
    "        mean_acc_group_df[\"accuracy\"] = mean_acc\n",
    "        mean_acc_rows.append(mean_acc_group_df)\n",
    "\n",
    "    return pd.concat(mean_acc_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_in_domain_df(df, train_col):\n",
    "    return df[df[train_col] == df[\"test\"]]\n",
    "\n",
    "\n",
    "def get_ood_df(df, train_col):\n",
    "    return df[df[train_col] != df[\"test\"]]\n",
    "\n",
    "\n",
    "def get_acc_stats_for_df(df, print=False):\n",
    "    train_col = train_col_name_for_transfer_acc(df)\n",
    "    train_test_metric_dfs_dict = make_train_test_metric_dfs_dict(df, metric=\"accuracy\")\n",
    "    for model, method_df in train_test_metric_dfs_dict.items():\n",
    "        for method, num_orth_dirs_df in method_df.items():\n",
    "            for num_orth_dirs, (mean_df, std_df) in num_orth_dirs_df.items():\n",
    "                melt_df = pd.melt(mean_df.reset_index(), id_vars=[train_col])\n",
    "                in_domain_acc = melt_df[melt_df[train_col] == melt_df[\"test\"]][\n",
    "                    \"value\"\n",
    "                ].mean()\n",
    "                ood_acc = melt_df[melt_df[train_col] != melt_df[\"test\"]][\"value\"].mean()\n",
    "                exp_title = f\"{model} - {method}\"\n",
    "                if num_orth_dirs is not None:\n",
    "                    exp_title += f\" - {num_orth_dirs} Orthogonal Directions\"\n",
    "                if print:\n",
    "                    print(exp_title)\n",
    "                    print(f\"In-domain accuracy: {in_domain_acc:.4f}\")\n",
    "                    print(f\"OOD accuracy: {ood_acc:.4f}\")\n",
    "\n",
    "\n",
    "def compare_accuracies(\n",
    "    df1,\n",
    "    df2,\n",
    "    train_col1,\n",
    "    train_col2,\n",
    "    acc_threshold=0.7,\n",
    "    is_ccs_lr_in_span_oracle: bool = False,\n",
    "):\n",
    "    df1 = filter_train_equals_test_per_method(\n",
    "        df1, is_ccs_lr_in_span_oracle=is_ccs_lr_in_span_oracle\n",
    "    )\n",
    "    df2 = filter_train_equals_test_per_method(\n",
    "        df2, is_ccs_lr_in_span_oracle=is_ccs_lr_in_span_oracle\n",
    "    )\n",
    "    agg_df1 = compute_mean_std_metric_df(df1, metric=\"accuracy\")\n",
    "    agg_df2 = compute_mean_std_metric_df(df2, metric=\"accuracy\")\n",
    "\n",
    "    # Rename train columns of both DataFrames to \"train_\" to make it a unique\n",
    "    # column name that does not conflict with other columns. Do this before\n",
    "    # making the train-test matrix DataFrames in order to be able to melt them\n",
    "    # correctly later.\n",
    "    agg_df1 = agg_df1.rename(columns={train_col1: \"train_\"})\n",
    "    agg_df2 = agg_df2.rename(columns={train_col2: \"train_\"})\n",
    "\n",
    "    pivot_df1, _ = make_train_test_df(agg_df1, \"train_\")\n",
    "    pivot_df2, _ = make_train_test_df(agg_df2, \"train_\")\n",
    "\n",
    "    diff_pivot_df = pivot_df1 - pivot_df2\n",
    "    print(\"Number of NaNs in diff_pivot_df:\", diff_pivot_df.isna().sum().sum())\n",
    "    high_acc_diff_pivot_df = diff_pivot_df[\n",
    "        (pivot_df1 >= acc_threshold) | (pivot_df2 >= acc_threshold)\n",
    "    ]\n",
    "\n",
    "    diff_df = diff_pivot_df.reset_index().melt(\n",
    "        id_vars=[\"train_\"], value_name=\"accuracy\"\n",
    "    )\n",
    "    in_domain_diff_df = get_in_domain_df(diff_df, \"train_\")\n",
    "    ood_diff_df = get_ood_df(diff_df, \"train_\")\n",
    "\n",
    "    frac_acc1_better = (diff_df[\"accuracy\"] > 0).mean()\n",
    "    frac_acc1_better_in_domain = (in_domain_diff_df[\"accuracy\"] > 0).mean()\n",
    "    frac_acc1_better_ood = (ood_diff_df[\"accuracy\"] > 0).mean()\n",
    "\n",
    "    high_acc_diff_df = high_acc_diff_pivot_df.reset_index().melt(\n",
    "        id_vars=[\"train_\"], value_name=\"accuracy\"\n",
    "    )\n",
    "    high_acc_diff_df.dropna(subset=\"accuracy\", inplace=True)\n",
    "    high_acc_in_domain_diff_df = get_in_domain_df(high_acc_diff_df, \"train_\")\n",
    "    high_acc_ood_diff_df = get_ood_df(high_acc_diff_df, \"train_\")\n",
    "\n",
    "    high_acc_frac_acc1_better = (high_acc_diff_df[\"accuracy\"] > 0).mean()\n",
    "    high_acc_frac_acc1_better_in_domain = (\n",
    "        high_acc_in_domain_diff_df[\"accuracy\"] > 0\n",
    "    ).mean()\n",
    "    high_acc_frac_acc1_better_ood = (high_acc_ood_diff_df[\"accuracy\"] > 0).mean()\n",
    "\n",
    "    return dict(\n",
    "        frac_acc1_better=frac_acc1_better,\n",
    "        frac_acc1_better_in_domain=frac_acc1_better_in_domain,\n",
    "        frac_acc1_better_ood=frac_acc1_better_ood,\n",
    "        high_acc_frac_acc1_better=high_acc_frac_acc1_better,\n",
    "        high_acc_frac_acc1_better_in_domain=high_acc_frac_acc1_better_in_domain,\n",
    "        high_acc_frac_acc1_better_ood=high_acc_frac_acc1_better_ood,\n",
    "    )\n",
    "\n",
    "\n",
    "def get_train_to_orth_dirs_dict(paths):\n",
    "    train_to_orth_dirs = {}\n",
    "\n",
    "    for path in paths:\n",
    "        orthogonal_directions = np.load(path)\n",
    "        with open(Path(*path.parts[:-2]) / \"config.json\", \"r\") as f:\n",
    "            config = json.load(f)\n",
    "\n",
    "        sup_train_ds = config[\"labeled_datasets\"]\n",
    "        if isinstance(sup_train_ds, (list, tuple)):\n",
    "            assert len(sup_train_ds) == 1\n",
    "            sup_train_ds = sup_train_ds[0]\n",
    "\n",
    "        if sup_train_ds in train_to_orth_dirs:\n",
    "            raise ValueError(\n",
    "                f\"Duplicate orthogonal directions found for {sup_train_ds}\"\n",
    "            )\n",
    "        train_to_orth_dirs[sup_train_ds] = orthogonal_directions\n",
    "\n",
    "    return train_to_orth_dirs\n",
    "\n",
    "\n",
    "def make_inner_prods_df(paths) -> pd.DataFrame:\n",
    "    train_to_orth_dirs = get_train_to_orth_dirs_dict(paths)\n",
    "    rows = []\n",
    "    for ds1, ds2 in itertools.product(train_to_orth_dirs, repeat=2):\n",
    "        inner_prods = train_to_orth_dirs[ds1] @ train_to_orth_dirs[ds2].T\n",
    "        for i in range(inner_prods.shape[0]):\n",
    "            for j in range(inner_prods.shape[1]):\n",
    "                rows.append(\n",
    "                    {\n",
    "                        \"ds1\": ds1,\n",
    "                        \"ds2\": ds2,\n",
    "                        \"dir1_idx\": i,\n",
    "                        \"dir2_idx\": j,\n",
    "                        \"inner_prod\": inner_prods[i, j],\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    inner_prods_df = pd.DataFrame(rows)\n",
    "    return inner_prods_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_heatmap(\n",
    "    df: pd.DataFrame,\n",
    "    metric: str,\n",
    "    std_annot: bool = False,\n",
    "    heatmap_kwargs: Optional[dict] = None,\n",
    "    xlabel: str = \"Test Dataset\",\n",
    "    ylabel: str = \"Train Dataset\",\n",
    "    title: Optional[str] = None,\n",
    "    show: bool = True,\n",
    "):\n",
    "    mean_std_df_dict = make_train_test_metric_dfs_dict(df, metric=metric)\n",
    "    for model, method_dict in mean_std_df_dict.items():\n",
    "        for method, num_orth_dirs_dict in method_dict.items():\n",
    "            for num_orth_dirs, (\n",
    "                pivot_table_mean_ordered,\n",
    "                pivot_table_std_ordered,\n",
    "            ) in num_orth_dirs_dict.items():\n",
    "                # Combine mean and std into a single string for each cell annotation\n",
    "                annotations = pivot_table_mean_ordered.applymap(\"{:.2f}\".format)\n",
    "                if std_annot:\n",
    "                    annotations += \"+-\" + pivot_table_std_ordered.applymap(\n",
    "                        \"{:.2f}\".format\n",
    "                    )\n",
    "                    annot_size = 6\n",
    "                else:\n",
    "                    annot_size = 10\n",
    "\n",
    "                # Create the heatmap with custom annotations\n",
    "                # Make a figure if an axis is not provided.\n",
    "                if \"ax\" in heatmap_kwargs:\n",
    "                    ax = heatmap_kwargs.pop(\"ax\")\n",
    "                else:\n",
    "                    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "                heatmap_kwargs = heatmap_kwargs or {}\n",
    "                sns.heatmap(\n",
    "                    pivot_table_mean_ordered,\n",
    "                    annot=annotations,\n",
    "                    fmt=\"\",\n",
    "                    cmap=\"viridis\",\n",
    "                    cbar_kws={\"label\": f\"Mean {metric.capitalize()}\"},\n",
    "                    annot_kws={\"size\": annot_size},\n",
    "                    square=True,\n",
    "                    ax=ax,\n",
    "                    **heatmap_kwargs,\n",
    "                )\n",
    "\n",
    "                # Adding title and axis labels\n",
    "                num_orthogonal_directions_str = (\n",
    "                    f\" (num_orth_dirs={num_orth_dirs})\"\n",
    "                    if num_orth_dirs is not None\n",
    "                    else \"\"\n",
    "                )\n",
    "                title = (\n",
    "                    title\n",
    "                    or f\"{model} - {method}{num_orthogonal_directions_str}\"\n",
    "                )\n",
    "                ax.set_title(title)\n",
    "                ax.set_xlabel(xlabel)\n",
    "                ax.set_ylabel(ylabel)\n",
    "\n",
    "                # Show the plot\n",
    "                if show:\n",
    "                    plt.show()\n",
    "\n",
    "\n",
    "def heatmap_subplots(\n",
    "    dfs: list[pd.DataFrame],\n",
    "    titles: Optional[list[str]] = None,\n",
    "    filter_train_equals_test: bool = True,\n",
    "    dedup_ccs_train_labeled_cols: bool = True,\n",
    "):\n",
    "    num_cols = 2\n",
    "    num_rows = len(dfs) // num_cols\n",
    "    if len(dfs) % num_cols != 0:\n",
    "        num_rows += 1\n",
    "    fig, axs = plt.subplots(\n",
    "        num_rows, num_cols, figsize=(9 * num_cols, 9 * num_rows), squeeze=False\n",
    "    )\n",
    "\n",
    "    for i, df in enumerate(dfs):\n",
    "        row = i // num_cols\n",
    "        col = i % num_cols\n",
    "        ax = axs[row, col]\n",
    "\n",
    "        if filter_train_equals_test:\n",
    "            df = filter_train_equals_test_per_method(\n",
    "                df, is_ccs_lr_in_span_oracle=is_ccs_lr_in_span_oracle\n",
    "            )\n",
    "\n",
    "        if (\n",
    "            df[\"method\"].unique() == [\"CCS+LR\"]\n",
    "            and reduce_ccs_lr_method(df) == \"CCS\"\n",
    "            and dedup_ccs_train_labeled_cols\n",
    "        ):\n",
    "            df = compute_mean_train_test_acc_df_over_nonunique_cols(df)\n",
    "\n",
    "        # Calculate mean accuracy and standard deviation of accuracy for each combination\n",
    "        agg_df = compute_mean_std_metric_df(df, metric=\"accuracy\")\n",
    "        train_col = train_col_name_for_transfer_acc(agg_df)\n",
    "        in_domain_acc = agg_df[agg_df[train_col] == agg_df[\"test\"]][\"mean\"].mean()\n",
    "        ood_acc = agg_df[agg_df[train_col] != agg_df[\"test\"]][\"mean\"].mean()\n",
    "\n",
    "        title = (\n",
    "            f\"{titles[i]} | in-domain: {in_domain_acc:.4f} | OOD: {ood_acc:.4f}\"\n",
    "            if titles is not None\n",
    "            else None\n",
    "        )\n",
    "        show_cbar = True  # col == num_cols - 1\n",
    "        heatmap_kwargs = {\"ax\": ax, \"vmin\": 0.5, \"vmax\": 1, \"cbar\": show_cbar}\n",
    "        xlabel = \"Test Dataset\" if row == num_rows - 1 else \"\"\n",
    "        ylabel = \"Train Dataset\" if col == 0 else \"\"\n",
    "        plot_heatmap(\n",
    "            df,\n",
    "            \"accuracy\",\n",
    "            std_annot=False,\n",
    "            heatmap_kwargs=heatmap_kwargs,\n",
    "            xlabel=xlabel,\n",
    "            ylabel=ylabel,\n",
    "            title=title,\n",
    "            show=False,\n",
    "        )\n",
    "\n",
    "    plt.subplots_adjust(hspace=0.35, wspace=0.2)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_inner_prods_heatmap(df: pd.DataFrame, needs_pivot: bool = True):\n",
    "    plot_heatmap_from_df(\n",
    "        df,\n",
    "        needs_pivot=needs_pivot,\n",
    "        pivot_values=\"inner_prod\",\n",
    "        cbar_label=\"Inner Product\",\n",
    "        title=\"Inner Product of Orthogonal LR Directions\",\n",
    "    )\n",
    "\n",
    "\n",
    "def plot_heatmap_from_df(\n",
    "    df: pd.DataFrame,\n",
    "    needs_pivot: bool = True,\n",
    "    pivot_values: Optional[str] = None,\n",
    "    cbar_label: Optional[str] = None,\n",
    "    title: Optional[str] = None,\n",
    "):\n",
    "    if needs_pivot:\n",
    "        if pivot_values is None:\n",
    "            raise ValueError(\"If needs_pivot is True, pivot_values must be provided.\")\n",
    "        df = df.pivot(index=\"ds1\", columns=\"ds2\", values=pivot_values).reindex(\n",
    "            index=DATASET_ORDER, columns=DATASET_ORDER\n",
    "        )\n",
    "\n",
    "    annotations = df.applymap(\"{:.2f}\".format)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(\n",
    "        df,\n",
    "        annot=annotations,\n",
    "        fmt=\"\",\n",
    "        cmap=\"viridis\",\n",
    "        cbar_kws={\"label\": cbar_label} if cbar_label else {},\n",
    "        annot_kws={\"size\": 10},\n",
    "        vmin=0,\n",
    "        vmax=1,\n",
    "    )\n",
    "\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.xlabel(\"Dataset 1\")\n",
    "    plt.ylabel(\"Dataset 2\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_violin(df):\n",
    "    # Filter the DataFrame where train == test\n",
    "    filtered_df = df[df[\"train\"] == df[\"test\"]]\n",
    "\n",
    "    # Group by [model, prefix, train]\n",
    "    grouped = filtered_df.groupby(\n",
    "        [\"model\", \"train\", \"train_labeled\", \"layer\", \"location\"]\n",
    "    )\n",
    "\n",
    "    # For each group, generate a violin plot\n",
    "    for (model, train, train_labeled, layer, location), group in grouped:\n",
    "        num_prefixes = group.prefix.nunique()\n",
    "        fig, axs = plt.subplots(1, num_prefixes, figsize=(10 * num_prefixes, 6))\n",
    "        axs = np.atleast_1d(axs)\n",
    "\n",
    "        for i, prefix in enumerate(group.prefix.unique()):\n",
    "            ax = axs[i]\n",
    "            prefix_group = group[group.prefix == prefix]\n",
    "            sns.violinplot(x=\"method\", y=\"accuracy\", data=prefix_group, ax=ax)\n",
    "            ax.set_title(\n",
    "                f\"{model}, {prefix}, train={train}, train_labeled={train_labeled}, layer={layer}, location={location}\"\n",
    "            )\n",
    "            ax.set_xlabel(\"Method\")\n",
    "            ax.set_ylabel(\"Accuracy\")\n",
    "            ax.set_xticklabels(ax.get_xticklabels(), rotation=45)\n",
    "\n",
    "        min_ylim = min(ax.get_ylim()[0] for ax in axs)\n",
    "        max_ylim = max(ax.get_ylim()[1] for ax in axs)\n",
    "        min_ylim = max(0, min_ylim)\n",
    "        max_ylim = min(1, max_ylim)\n",
    "        for ax in axs:\n",
    "            ax.set_ylim(min_ylim, max_ylim)\n",
    "\n",
    "        fig.tight_layout()  # Adjust subplot parameters for the plot to fit into the figure area\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def plot_history(fit_result, vars_to_plot=None, one_fig=True):\n",
    "    if not fit_result[\"histories\"]:\n",
    "        raise ValueError(\"No histories found in fit_result\")\n",
    "\n",
    "    if vars_to_plot is None:\n",
    "        vars_to_plot = [\"total_loss\", \"supervised_loss\", \"unsupervised_loss\"]\n",
    "    else:\n",
    "        if not all(var_name in fit_result[\"histories\"][0] for var_name in vars_to_plot):\n",
    "            raise ValueError(\n",
    "                f\"vars_to_plot contains unknown variable names: {vars_to_plot}\"\n",
    "            )\n",
    "\n",
    "    # Prepare the data for plotting\n",
    "    data = []\n",
    "    for var_name in vars_to_plot:\n",
    "        for trial, history in enumerate(fit_result[\"histories\"]):\n",
    "            var_history = history.get(var_name, [])\n",
    "            for epoch, value in enumerate(var_history):\n",
    "                data.append(\n",
    "                    {\n",
    "                        \"Epoch\": epoch,\n",
    "                        \"Value\": value,\n",
    "                        \"Type\": var_name,\n",
    "                        \"Trial\": trial,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Plot\n",
    "    nrows = 1 if one_fig else len(vars_to_plot)\n",
    "    fig, axs = plt.subplots(nrows, 1, figsize=(8, nrows * 6))\n",
    "\n",
    "    if one_fig:\n",
    "        sns.lineplot(\n",
    "            data=df,\n",
    "            x=\"Epoch\",\n",
    "            y=\"Value\",\n",
    "            hue=\"Type\",\n",
    "            style=\"Type\",\n",
    "            ci=\"sd\",\n",
    "            markers=False,\n",
    "            dashes=False,\n",
    "            ax=axs,\n",
    "        )\n",
    "        axs.set_title(\"History with Confidence Interval\")\n",
    "        axs.set_xlabel(\"Epoch\")\n",
    "        axs.legend(title=\"Value\")\n",
    "    else:\n",
    "        if nrows == 1:\n",
    "            axs = [axs]\n",
    "\n",
    "        for ax, var_name in zip(axs, vars_to_plot):\n",
    "            sns.lineplot(\n",
    "                data=df[df[\"Type\"] == var_name],\n",
    "                x=\"Epoch\",\n",
    "                y=\"Value\",\n",
    "                ci=\"sd\",\n",
    "                markers=False,\n",
    "                dashes=False,\n",
    "                ax=ax,\n",
    "            )\n",
    "            ax.set_title(f\"{var_name} History\")\n",
    "            ax.set_xlabel(\"Epoch\")\n",
    "            ax.set_ylabel(var_name)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def calibration_plot(probabilities, labels, num_bins=10, title=\"Calibration Plot\"):\n",
    "    \"\"\"\n",
    "    Create a calibration plot using Seaborn.\n",
    "\n",
    "    Args:\n",
    "    - probabilities (array-like): Predicted probabilities for each sample, in the range [0, 1].\n",
    "    - labels (array-like): True labels for each sample, in {0, 1}.\n",
    "    - num_bins (int): Number of bins to divide the interval [0, 1].\n",
    "    \"\"\"\n",
    "    ece, bin_mean_probs, bin_mean_labels = expected_calibration_error(\n",
    "        probabilities, labels, num_bins=num_bins\n",
    "    )\n",
    "    calibration_plot_from_bin_xy(\n",
    "        bin_mean_probs, bin_mean_labels, title=f\"{title} (ECE: {ece:.4f})\"\n",
    "    )\n",
    "\n",
    "\n",
    "def calibration_plot_from_bin_xy(\n",
    "    bin_mean_probs, bin_mean_labels, title=\"Calibration Plot\", ax=None\n",
    "):\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    sns.lineplot(x=bin_mean_probs, y=bin_mean_labels, marker=\"o\", ax=ax)\n",
    "    ax.plot([0, 1], [0, 1], \"k--\")  # Perfect calibration line\n",
    "    ax.set_xlabel(\"Mean Predicted Probability\")\n",
    "    ax.set_ylabel(\"Mean Label\")\n",
    "    ax.set_title(title)\n",
    "    ax.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Individual experiment names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_names = [\n",
    "    # \"Llama-2-7b-chat-hf_normal-bananashed_LR\",\n",
    "    # \"Llama-2-7b-chat-hf_normal-bananashed_CCS+LR_transfer_sup_weight=3-unsup_weight=1-lr=1e-2-n_epochs=1000\",\n",
    "    # \"Llama-2-7b-chat-hf_normal-bananashed_LR-mode=concat\",\n",
    "    # \"Llama-2-7b-chat-hf_normal-bananashed_CCS+LR_sweep\",\n",
    "    # \"Llama-2-7b-chat-hf_normal-bananashed_CCS+LR_adam\",\n",
    "    # \"Llama-2-7b-chat-hf_normal-bananashed_CCS+LR/sup_weight=10-unsup_weight=1-lr=1e-3-n_epochs=1000\",\n",
    "    # \"Llama-2-7b-chat-hf_normal-bananashed_LR-mode=concat-sweep\",\n",
    "    # \"Llama-2-7b-chat-hf_train_prefix_normal-test_prefix_normal-bananashed_LR-mode=concat\",\n",
    "    # \"Llama-2-7b-chat-hf_train_prefix_normal-test_prefix_normal-bananashed_CCS+LR\",\n",
    "    # \"Llama-2-7b-chat-hf_normal-bananashed_CCS-in-LR-span_debug\",\n",
    "    # \"Llama-2-7b-chat-hf_normal-bananashed_CCS-in-LR-span_debug/meta-llama-Llama-2-7b-chat-hf/nolabel_dbpedia-14-label_imdb/seed_0/20\",\n",
    "    # \"Llama-2-7b-chat-hf_normal-bananashed_CCS+LR_debug\",\n",
    "    # \"Llama-2-7b-chat-hf_normal-bananashed_CCS-select-LR-sweep_num_dirs\",\n",
    "    # \"Llama-2-7b-chat-hf_normal-bananashed_CCS-in-LR-span-first-4-datasets\",\n",
    "    # \"Llama-2-7b-chat-hf_normal-bananashed_CCS-in-LR-span-convex\",\n",
    "    # \"Llama-2-7b-chat-hf_train-normal_test-bananashed_CCS-in-LR-span-convex\",\n",
    "    # \"Llama-2-7b-chat-hf_train-normal_test-bananashed_CCS-in-LR-span-affine\",\n",
    "    # \"Llama-2-7b-chat-hf_normal_CCS-in-LR-span-affine\",\n",
    "    # \"Llama-2-7b-chat-hf_normal-bananashed_LR-C=0.01-max_iter=10k-penalty=l2\",\n",
    "    # \"Llama-2-7b-chat-hf_normal_CCS-in-LR-span-convex_2_orth_dirs\",\n",
    "    # \"Llama-2-7b-chat-hf_normal_CCS-in-LR-span-convex_4_orth_dirs\",\n",
    "    # \"Llama-2-7b-chat-hf_normal_CCS-in-LR-span-convex_8_orth_dirs\",\n",
    "    # \"Llama-2-7b-chat-hf_normal_CCS-in-LR-span-convex_10_orth_dirs\",\n",
    "    # \"Llama-2-7b-chat-hf_normal_CCS-in-LR-span-convex_15_orth_dirs\",\n",
    "    # \"Llama-2-7b-chat-hf_normal_CCS-in-LR-span-convex_20_orth_dirs\",\n",
    "    # \"Llama-2-7b-chat-hf_normal-bananashed_CCS-in-LR-span-convex_20_orth_dirs\",\n",
    "    # \"Llama-2-7b-chat-hf_normal-bananashed_oracle-LR-in-LR-span-convex_1_orth_dirs\",\n",
    "    # \"Llama-2-7b-chat-hf_normal-bananashed_oracle-LR-in-LR-span-convex_2_orth_dirs\",\n",
    "    # \"Llama-2-7b-chat-hf_normal-bananashed_oracle-LR-in-LR-span-convex_4_orth_dirs\",\n",
    "    # \"Llama-2-7b-chat-hf_normal-bananashed_oracle-LR-in-LR-span-convex_8_orth_dirs\",\n",
    "    # \"Llama-2-7b-chat-hf_normal-bananashed_oracle-LR-in-LR-span-convex_15_orth_dirs\",\n",
    "    # \"Llama-2-7b-chat-hf_normal-bananashed_oracle-LR-in-LR-span-convex_20_orth_dirs\",\n",
    "    # \"Llama-2-7b-chat-hf_normal-bananashed_LR-in-CCS-span-convex_1_orth_dirs-v2\",\n",
    "    # \"Llama-2-7b-chat-hf_normal-bananashed_LR-in-CCS-span-convex_2_orth_dirs-v2\",\n",
    "    # \"Llama-2-7b-chat-hf_normal-bananashed_LR-in-CCS-span-convex_4_orth_dirs-v2\",\n",
    "    # \"Llama-2-7b-chat-hf_normal-bananashed_LR-in-CCS-span-convex_8_orth_dirs-v2\",\n",
    "    # \"Llama-2-7b-chat-hf_normal-bananashed_LR-in-CCS-span-convex_15_orth_dirs-v2\",\n",
    "    # \"Llama-2-7b-chat-hf_normal-bananashed_LR-in-CCS-span-convex_20_orth_dirs\",\n",
    "    # \"Llama-2-7b-chat-hf_normal-bananashed_LR-in-CCS-span-convex_100_orth_dirs\",\n",
    "\n",
    "    ### LR (CCS+LR implementation)\n",
    "\n",
    "    ## normal prompt\n",
    "    # \"llama-2-7b-chat-hf/normal/ccs_lr/mode_concat/sup_weight_1/unsup_weight_0/lr_0.01/n_epochs_5000/meta-llama-Llama-2-7b-chat-hf\",\n",
    "    ## normal-bananashed prompt\n",
    "    # \"llama-2-7b-chat-hf/normal-bananashed/ccs_lr/mode_concat/sup_weight_1/unsup_weight_0/lr_0.01/n_epochs_5000/meta-llama-Llama-2-7b-chat-hf\",\n",
    "\n",
    "    ### Pseudo-label\n",
    "\n",
    "    ## select_fn=high_confidence_consistency, label_fn=argmax, normal prompt\n",
    "    # \"llama-2-7b-chat-hf/normal/pseudolabel/rounds_5/select_high_confidence_consistency/prob_thres_0.5/label_argmax\",\n",
    "    # \"llama-2-7b-chat-hf/normal/pseudolabel/rounds_5/select_high_confidence_consistency/prob_thres_0.6/label_argmax\",\n",
    "    # \"llama-2-7b-chat-hf/normal/pseudolabel/rounds_5/select_high_confidence_consistency/prob_thres_0.7/label_argmax\",\n",
    "    # \"llama-2-7b-chat-hf/normal/pseudolabel/rounds_5/select_high_confidence_consistency/prob_thres_0.8/label_argmax\",\n",
    "    # \"llama-2-7b-chat-hf/normal/pseudolabel/rounds_5/select_high_confidence_consistency/prob_thres_0.9/label_argmax\",\n",
    "\n",
    "    ## select_fn=high_confidence_consistency, label_fn=argmax, normal-bananashed prompt\n",
    "    # \"llama-2-7b-chat-hf/normal-bananashed/pseudolabel/rounds_5/select_high_confidence_consistency/prob_thres_0.5/label_argmax\",\n",
    "    # \"llama-2-7b-chat-hf/normal-bananashed/pseudolabel/rounds_5/select_high_confidence_consistency/prob_thres_0.6/label_argmax\",\n",
    "    # \"llama-2-7b-chat-hf/normal-bananashed/pseudolabel/rounds_5/select_high_confidence_consistency/prob_thres_0.7/label_argmax\",\n",
    "    # \"llama-2-7b-chat-hf/normal-bananashed/pseudolabel/rounds_5/select_high_confidence_consistency/prob_thres_0.8/label_argmax\",\n",
    "    # \"llama-2-7b-chat-hf/normal-bananashed/pseudolabel/rounds_5/select_high_confidence_consistency/prob_thres_0.9/label_argmax\",\n",
    "\n",
    "    ## select_fn=all, label_fn=argmax, normal prompt\n",
    "    # \"llama-2-7b-chat-hf/normal/pseudolabel/rounds_1/select_all/label_argmax\",\n",
    "\n",
    "    ## select_fn=all, label_fn=argmax, normal-bananashed prompt\n",
    "    # \"llama-2-7b-chat-hf/normal-bananashed/pseudolabel/rounds_1/select_all/label_argmax/meta-llama-Llama-2-7b-chat-hf\",\n",
    "\n",
    "    ### Llama 2 7B, LR (CCS+LR impl), normal prompt, layer sweep\n",
    "    # \"llama-2-7b-chat-hf/normal/layer_-1/ccs_lr/mode_concat/sup_weight_1/unsup_weight_0/lr_0.01/n_epochs_5000/meta-llama-Llama-2-7b-chat-hf\",\n",
    "    # \"llama-2-7b-chat-hf/normal/layer_-3/ccs_lr/mode_concat/sup_weight_1/unsup_weight_0/lr_0.01/n_epochs_5000/meta-llama-Llama-2-7b-chat-hf\",\n",
    "    # \"llama-2-7b-chat-hf/normal/layer_-5/ccs_lr/mode_concat/sup_weight_1/unsup_weight_0/lr_0.01/n_epochs_5000/meta-llama-Llama-2-7b-chat-hf\",\n",
    "    # \"llama-2-7b-chat-hf/normal/layer_-7/ccs_lr/mode_concat/sup_weight_1/unsup_weight_0/lr_0.01/n_epochs_5000/meta-llama-Llama-2-7b-chat-hf\",\n",
    "    # \"llama-2-7b-chat-hf/normal/layer_-9/ccs_lr/mode_concat/sup_weight_1/unsup_weight_0/lr_0.01/n_epochs_5000/meta-llama-Llama-2-7b-chat-hf\",\n",
    "\n",
    "    ### Llama 2 7B, LR (CCS+LR impl), bananashed prompt, layer sweep\n",
    "    # \"llama-2-7b-chat-hf/normal-bananashed/layer_-1/ccs_lr/mode_concat/sup_weight_1/unsup_weight_0/lr_0.01/n_epochs_5000/meta-llama-Llama-2-7b-chat-hf\",\n",
    "    # \"llama-2-7b-chat-hf/normal-bananashed/layer_-3/ccs_lr/mode_concat/sup_weight_1/unsup_weight_0/lr_0.01/n_epochs_5000/meta-llama-Llama-2-7b-chat-hf\",\n",
    "    # \"llama-2-7b-chat-hf/normal-bananashed/layer_-5/ccs_lr/mode_concat/sup_weight_1/unsup_weight_0/lr_0.01/n_epochs_5000/meta-llama-Llama-2-7b-chat-hf\",\n",
    "    # \"llama-2-7b-chat-hf/normal-bananashed/layer_-7/ccs_lr/mode_concat/sup_weight_1/unsup_weight_0/lr_0.01/n_epochs_5000/meta-llama-Llama-2-7b-chat-hf\",\n",
    "    # \"llama-2-7b-chat-hf/normal-bananashed/layer_-9/ccs_lr/mode_concat/sup_weight_1/unsup_weight_0/lr_0.01/n_epochs_5000/meta-llama-Llama-2-7b-chat-hf\",\n",
    "\n",
    "    ### Llama 2 13B, normal prompt, LR (CCS+LR impl)\n",
    "    # \"llama-2-13b-chat-hf/normal/layer_-1/ccs_lr/mode_concat/sup_weight_1/unsup_weight_0/lr_0.01/n_epochs_5000/meta-llama-Llama-2-13b-chat-hf\",\n",
    "    # \"llama-2-13b-chat-hf/normal/layer_-3/ccs_lr/mode_concat/sup_weight_1/unsup_weight_0/lr_0.01/n_epochs_5000/meta-llama-Llama-2-13b-chat-hf\",\n",
    "    # \"llama-2-13b-chat-hf/normal/layer_-5/ccs_lr/mode_concat/sup_weight_1/unsup_weight_0/lr_0.01/n_epochs_5000/meta-llama-Llama-2-13b-chat-hf\",\n",
    "    # \"llama-2-13b-chat-hf/normal/layer_-7/ccs_lr/mode_concat/sup_weight_1/unsup_weight_0/lr_0.01/n_epochs_5000/meta-llama-Llama-2-13b-chat-hf\",\n",
    "    # \"llama-2-13b-chat-hf/normal/layer_-9/ccs_lr/mode_concat/sup_weight_1/unsup_weight_0/lr_0.01/n_epochs_5000/meta-llama-Llama-2-13b-chat-hf\",\n",
    "\n",
    "    ### Llama 2 13B, normal prompt, CCS\n",
    "    # \"llama-2-13b-chat-hf/normal/layer_-1/ccs_lr/mode_concat/sup_weight_0/unsup_weight_1/lr_0.01/n_epochs_5000/meta-llama-Llama-2-13b-chat-hf\",\n",
    "    # \"llama-2-13b-chat-hf/normal/layer_-3/ccs_lr/mode_concat/sup_weight_0/unsup_weight_1/lr_0.01/n_epochs_5000/meta-llama-Llama-2-13b-chat-hf\",\n",
    "    # \"llama-2-13b-chat-hf/normal/layer_-5/ccs_lr/mode_concat/sup_weight_0/unsup_weight_1/lr_0.01/n_epochs_5000/meta-llama-Llama-2-13b-chat-hf\",\n",
    "    # \"llama-2-13b-chat-hf/normal/layer_-7/ccs_lr/mode_concat/sup_weight_0/unsup_weight_1/lr_0.01/n_epochs_5000/meta-llama-Llama-2-13b-chat-hf\",\n",
    "    # \"llama-2-13b-chat-hf/normal/layer_-9/ccs_lr/mode_concat/sup_weight_0/unsup_weight_1/lr_0.01/n_epochs_5000/meta-llama-Llama-2-13b-chat-hf\",\n",
    "\n",
    "    ### Llama 2 13B, normal prompt, pseudolabel select_fn=select_high_confidence_consistency label_fn=argmax\n",
    "    # \"llama-2-13b-chat-hf/normal/layer_-1/pseudolabel/rounds_5/select_high_confidence_consistency/prob_thres_0.7/label_argmax/meta-llama-Llama-2-13b-chat-hf\",\n",
    "    # \"llama-2-13b-chat-hf/normal/layer_-3/pseudolabel/rounds_5/select_high_confidence_consistency/prob_thres_0.7/label_argmax/meta-llama-Llama-2-13b-chat-hf\",\n",
    "    # \"llama-2-13b-chat-hf/normal/layer_-5/pseudolabel/rounds_5/select_high_confidence_consistency/prob_thres_0.7/label_argmax/meta-llama-Llama-2-13b-chat-hf\",\n",
    "    # \"llama-2-13b-chat-hf/normal/layer_-7/pseudolabel/rounds_5/select_high_confidence_consistency/prob_thres_0.7/label_argmax/meta-llama-Llama-2-13b-chat-hf\",\n",
    "    # \"llama-2-13b-chat-hf/normal/layer_-9/pseudolabel/rounds_5/select_high_confidence_consistency/prob_thres_0.7/label_argmax/meta-llama-Llama-2-13b-chat-hf\",\n",
    "\n",
    "    ### Llama 2 13B, normal prompt, pseudolabel select_fn=all label_fn=argmax\n",
    "    # \"llama-2-13b-chat-hf/normal/layer_-1/pseudolabel/rounds_1/select_all/label_argmax/meta-llama-Llama-2-13b-chat-hf\",\n",
    "    # \"llama-2-13b-chat-hf/normal/layer_-3/pseudolabel/rounds_1/select_all/label_argmax/meta-llama-Llama-2-13b-chat-hf\",\n",
    "    # \"llama-2-13b-chat-hf/normal/layer_-5/pseudolabel/rounds_1/select_all/label_argmax/meta-llama-Llama-2-13b-chat-hf\",\n",
    "    # \"llama-2-13b-chat-hf/normal/layer_-7/pseudolabel/rounds_1/select_all/label_argmax/meta-llama-Llama-2-13b-chat-hf\",\n",
    "    # \"llama-2-13b-chat-hf/normal/layer_-9/pseudolabel/rounds_1/select_all/label_argmax/meta-llama-Llama-2-13b-chat-hf\",\n",
    "\n",
    "    ### Llama 3 8B, normal prompt, CCS\n",
    "    # \"meta-llama/Meta-Llama-3-8B/normal/layer_-1/ccs_lr/mode_concat/sup_weight_0/unsup_weight_1/lr_0.01/n_epochs_5000/meta-llama-Meta-Llama-3-8B\",\n",
    "    # \"meta-llama/Meta-Llama-3-8B/normal/layer_-1/ccs_lr/mode_concat/sup_weight_0/unsup_weight_1/lr_0.01/n_epochs_5000/meta-llama-Meta-Llama-3-8B\",\n",
    "    # \"meta-llama/Meta-Llama-3-8B/normal/layer_-1/ccs_lr/mode_concat/sup_weight_0/unsup_weight_1/lr_0.01/n_epochs_5000/meta-llama-Meta-Llama-3-8B\",\n",
    "    # \"meta-llama/Meta-Llama-3-8B/normal/layer_-1/ccs_lr/mode_concat/sup_weight_0/unsup_weight_1/lr_0.01/n_epochs_5000/meta-llama-Meta-Llama-3-8B\",\n",
    "    # \"meta-llama/Meta-Llama-3-8B/normal/layer_-1/ccs_lr/mode_concat/sup_weight_0/unsup_weight_1/lr_0.01/n_epochs_5000/meta-llama-Meta-Llama-3-8B\",\n",
    "\n",
    "    ### Llama 3 8B, normal prompt, LR (CCS+LR impl)\n",
    "    # \"meta-llama/Meta-Llama-3-8B/normal/layer_-1/ccs_lr/mode_concat/sup_weight_1/unsup_weight_0/lr_0.01/n_epochs_5000/meta-llama-Meta-Llama-3-8B\",\n",
    "    # \"meta-llama/Meta-Llama-3-8B/normal/layer_-1/ccs_lr/mode_concat/sup_weight_1/unsup_weight_0/lr_0.01/n_epochs_5000/meta-llama-Meta-Llama-3-8B\",\n",
    "    # \"meta-llama/Meta-Llama-3-8B/normal/layer_-1/ccs_lr/mode_concat/sup_weight_1/unsup_weight_0/lr_0.01/n_epochs_5000/meta-llama-Meta-Llama-3-8B\",\n",
    "    # \"meta-llama/Meta-Llama-3-8B/normal/layer_-1/ccs_lr/mode_concat/sup_weight_1/unsup_weight_0/lr_0.01/n_epochs_5000/meta-llama-Meta-Llama-3-8B\",\n",
    "    # \"meta-llama/Meta-Llama-3-8B/normal/layer_-1/ccs_lr/mode_concat/sup_weight_1/unsup_weight_0/lr_0.01/n_epochs_5000/meta-llama-Meta-Llama-3-8B\",\n",
    "\n",
    "    ### Llama 3 8B, normal prompt, pseudolabel select_fn=select_high_confidence_consistency label_fn=argmax\n",
    "    # \"meta-llama/Meta-Llama-3-8B/normal/layer_-1/pseudolabel/rounds_5/select_high_confidence_consistency/prob_thres_0.7/label_argmax/meta-llama-Meta-Llama-3-8B\",\n",
    "    # \"meta-llama/Meta-Llama-3-8B/normal/layer_-3/pseudolabel/rounds_5/select_high_confidence_consistency/prob_thres_0.7/label_argmax/meta-llama-Meta-Llama-3-8B\",\n",
    "    # \"meta-llama/Meta-Llama-3-8B/normal/layer_-5/pseudolabel/rounds_5/select_high_confidence_consistency/prob_thres_0.7/label_argmax/meta-llama-Meta-Llama-3-8B\",\n",
    "    # \"meta-llama/Meta-Llama-3-8B/normal/layer_-7/pseudolabel/rounds_5/select_high_confidence_consistency/prob_thres_0.7/label_argmax/meta-llama-Meta-Llama-3-8B\",\n",
    "    # \"meta-llama/Meta-Llama-3-8B/normal/layer_-9/pseudolabel/rounds_5/select_high_confidence_consistency/prob_thres_0.7/label_argmax/meta-llama-Meta-Llama-3-8B\",\n",
    "\n",
    "    ### Llama 3 8B, normal prompt, pseudolabel select_fn=all label_fn=argmax\n",
    "    # \"meta-llama/Meta-Llama-3-8B/normal/layer_-1/pseudolabel/rounds_1/select_all/label_argmax/meta-llama-Meta-Llama-3-8B\",\n",
    "    # \"meta-llama/Meta-Llama-3-8B/normal/layer_-3/pseudolabel/rounds_1/select_all/label_argmax/meta-llama-Meta-Llama-3-8B\",\n",
    "    # \"meta-llama/Meta-Llama-3-8B/normal/layer_-5/pseudolabel/rounds_1/select_all/label_argmax/meta-llama-Meta-Llama-3-8B\",\n",
    "    # \"meta-llama/Meta-Llama-3-8B/normal/layer_-7/pseudolabel/rounds_1/select_all/label_argmax/meta-llama-Meta-Llama-3-8B\",\n",
    "    # \"meta-llama/Meta-Llama-3-8B/normal/layer_-9/pseudolabel/rounds_1/select_all/label_argmax/meta-llama-Meta-Llama-3-8B\",\n",
    "\n",
    "    ### Mistral 7B, normal prompt, CCS\n",
    "    # \"mistralai/Mistral-7B-Instruct-v0.2/normal/layer_-1/ccs_lr/mode_concat/sup_weight_0/unsup_weight_1/lr_0.01/n_epochs_5000/mistralai-Mistral-7B-Instruct-v0.2\",\n",
    "    # \"mistralai/Mistral-7B-Instruct-v0.2/normal/layer_-3/ccs_lr/mode_concat/sup_weight_0/unsup_weight_1/lr_0.01/n_epochs_5000/mistralai-Mistral-7B-Instruct-v0.2\",\n",
    "    # \"mistralai/Mistral-7B-Instruct-v0.2/normal/layer_-5/ccs_lr/mode_concat/sup_weight_0/unsup_weight_1/lr_0.01/n_epochs_5000/mistralai-Mistral-7B-Instruct-v0.2\",\n",
    "    # \"mistralai/Mistral-7B-Instruct-v0.2/normal/layer_-7/ccs_lr/mode_concat/sup_weight_0/unsup_weight_1/lr_0.01/n_epochs_5000/mistralai-Mistral-7B-Instruct-v0.2\",\n",
    "    # \"mistralai/Mistral-7B-Instruct-v0.2/normal/layer_-9/ccs_lr/mode_concat/sup_weight_0/unsup_weight_1/lr_0.01/n_epochs_5000/mistralai-Mistral-7B-Instruct-v0.2\",\n",
    "\n",
    "    ### Mistral 7B, normal prompt, LR (CCS+LR impl)\n",
    "    # \"mistralai/Mistral-7B-Instruct-v0.2/normal/layer_-1/ccs_lr/mode_concat/sup_weight_1/unsup_weight_0/lr_0.01/n_epochs_5000/mistralai-Mistral-7B-Instruct-v0.2\",\n",
    "    # \"mistralai/Mistral-7B-Instruct-v0.2/normal/layer_-3/ccs_lr/mode_concat/sup_weight_1/unsup_weight_0/lr_0.01/n_epochs_5000/mistralai-Mistral-7B-Instruct-v0.2\",\n",
    "    # \"mistralai/Mistral-7B-Instruct-v0.2/normal/layer_-5/ccs_lr/mode_concat/sup_weight_1/unsup_weight_0/lr_0.01/n_epochs_5000/mistralai-Mistral-7B-Instruct-v0.2\",\n",
    "    # \"mistralai/Mistral-7B-Instruct-v0.2/normal/layer_-7/ccs_lr/mode_concat/sup_weight_1/unsup_weight_0/lr_0.01/n_epochs_5000/mistralai-Mistral-7B-Instruct-v0.2\",\n",
    "    # \"mistralai/Mistral-7B-Instruct-v0.2/normal/layer_-9/ccs_lr/mode_concat/sup_weight_1/unsup_weight_0/lr_0.01/n_epochs_5000/mistralai-Mistral-7B-Instruct-v0.2\",\n",
    "\n",
    "    ### Mistral 7B, normal prompt, pseudolabel select_fn=select_high_confidence_consistency label_fn=argmax\n",
    "    # \"mistralai/Mistral-7B-Instruct-v0.2/normal/layer_-1/pseudolabel/rounds_5/select_high_confidence_consistency/prob_thres_0.7/label_argmax/mistralai-Mistral-7B-Instruct-v0.2\",\n",
    "    # \"mistralai/Mistral-7B-Instruct-v0.2/normal/layer_-3/pseudolabel/rounds_5/select_high_confidence_consistency/prob_thres_0.7/label_argmax/mistralai-Mistral-7B-Instruct-v0.2\",\n",
    "    # \"mistralai/Mistral-7B-Instruct-v0.2/normal/layer_-5/pseudolabel/rounds_5/select_high_confidence_consistency/prob_thres_0.7/label_argmax/mistralai-Mistral-7B-Instruct-v0.2\",\n",
    "    # \"mistralai/Mistral-7B-Instruct-v0.2/normal/layer_-7/pseudolabel/rounds_5/select_high_confidence_consistency/prob_thres_0.7/label_argmax/mistralai-Mistral-7B-Instruct-v0.2\",\n",
    "    # \"mistralai/Mistral-7B-Instruct-v0.2/normal/layer_-9/pseudolabel/rounds_5/select_high_confidence_consistency/prob_thres_0.7/label_argmax/mistralai-Mistral-7B-Instruct-v0.2\",\n",
    "\n",
    "    ### Mistral 7B, normal prompt, pseudolabel select_fn=all label_fn=argmax\n",
    "    # \"mistralai/Mistral-7B-Instruct-v0.2/normal/layer_-1/pseudolabel/rounds_1/select_all/label_argmax/mistralai-Mistral-7B-Instruct-v0.2\",\n",
    "    # \"mistralai/Mistral-7B-Instruct-v0.2/normal/layer_-3/pseudolabel/rounds_1/select_all/label_argmax/mistralai-Mistral-7B-Instruct-v0.2\",\n",
    "    # \"mistralai/Mistral-7B-Instruct-v0.2/normal/layer_-5/pseudolabel/rounds_1/select_all/label_argmax/mistralai-Mistral-7B-Instruct-v0.2\",\n",
    "    # \"mistralai/Mistral-7B-Instruct-v0.2/normal/layer_-7/pseudolabel/rounds_1/select_all/label_argmax/mistralai-Mistral-7B-Instruct-v0.2\",\n",
    "    # \"mistralai/Mistral-7B-Instruct-v0.2/normal/layer_-9/pseudolabel/rounds_1/select_all/label_argmax/mistralai-Mistral-7B-Instruct-v0.2\",\n",
    "]\n",
    "\n",
    "# Example filters (set to None if you don't want to filter by these)\n",
    "model_filter = None\n",
    "train_set_filter = None\n",
    "eval_set_filter = None\n",
    "use_latest_run_id = True\n",
    "use_best_in_domain_acc = True\n",
    "is_ccs_lr_in_span_oracle = False\n",
    "\n",
    "save_dir = Path(\"extraction_results\")\n",
    "\n",
    "# Load the evaluation CSVs\n",
    "experiment_dfs = {}\n",
    "for experiment_name in tqdm(experiment_names, desc=\"Experiments\"):\n",
    "    experiment_dfs[experiment_name] = load_eval_csvs(\n",
    "        save_dir / experiment_name,\n",
    "        model_filter=model_filter,\n",
    "        train_set_filter=train_set_filter,\n",
    "        eval_set_filter=eval_set_filter,\n",
    "        use_latest_run_id=use_latest_run_id,\n",
    "        use_best_in_domain_acc=use_best_in_domain_acc,\n",
    "        is_ccs_lr_in_span_oracle=is_ccs_lr_in_span_oracle,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments base path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example filters (set to None if you don't want to filter by these)\n",
    "model_filter = None\n",
    "train_set_filter = None\n",
    "eval_set_filter = None\n",
    "use_latest_run_id = True\n",
    "use_best_in_domain_acc = True\n",
    "is_ccs_lr_in_span_oracle = False\n",
    "\n",
    "experiment_dfs = load_eval_csvs(\n",
    "    \"extraction_results/llama-2-13b-chat-hf/normal/layer_-1/ccs_lr_in_span/mode_concat/sup_weight_1/unsup_weight_0/lr_0.01/n_epochs_5000\",\n",
    "    pattern=None,\n",
    "    model_filter=model_filter,\n",
    "    train_set_filter=train_set_filter,\n",
    "    eval_set_filter=eval_set_filter,\n",
    "    use_latest_run_id=use_latest_run_id,\n",
    "    use_best_in_domain_acc=use_best_in_domain_acc,\n",
    "    is_ccs_lr_in_span_oracle=is_ccs_lr_in_span_oracle,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in sorted(list(experiment_dfs.keys())):\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot multiple transfer accuracy heatmaps in the same figure.\n",
    "layers = []\n",
    "for p, df in experiment_dfs.items():\n",
    "    layer = int(p.split(\"/layer_\")[1].split(\"/\")[0])\n",
    "    if layer < 0:\n",
    "        model = df[\"model\"].unique()\n",
    "        if len(model) > 1:\n",
    "            raise ValueError(f\"Multiple models found: {model}\")\n",
    "        num_hidden_layers = hf_utils.get_num_hidden_layers(\n",
    "            model[0], use_auth_token=HF_AUTH_TOKEN\n",
    "        )\n",
    "        layer += num_hidden_layers\n",
    "    layers.append(layer)\n",
    "\n",
    "layers_exp_df_items = sorted(zip(layers, experiment_dfs.items()), key=lambda x: x[0])\n",
    "dfs = [df for _, (_, df) in layers_exp_df_items]\n",
    "titles = [f\"layer {layer}\" for layer, (_, df) in layers_exp_df_items]\n",
    "\n",
    "filter_train_equals_test = True\n",
    "# If we accidentally evaluated CCS on multiple train_labeled columns per\n",
    "# (train, test) pair, take the mean accuracy over the train_labeled column per\n",
    "# pair, which should be identical (up to randomness in the training).\n",
    "dedup_ccs_train_labeled_cols = True\n",
    "\n",
    "heatmap_subplots(dfs, titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Individually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_train_equals_test = True\n",
    "# If we accidentally evaluated CCS on multiple train_labeled columns per\n",
    "# (train, test) pair, take the mean accuracy over the train_labeled column per\n",
    "# pair, which should be identical (up to randomness in the training).\n",
    "dedup_ccs_train_labeled_cols = True\n",
    "\n",
    "for exp_name, df in experiment_dfs.items():\n",
    "    print(exp_name)\n",
    "    if filter_train_equals_test:\n",
    "        df = filter_train_equals_test_per_method(\n",
    "            df, is_ccs_lr_in_span_oracle=is_ccs_lr_in_span_oracle\n",
    "        )\n",
    "\n",
    "    if (\n",
    "        df[\"method\"].unique() == [\"CCS+LR\"]\n",
    "        and reduce_ccs_lr_method(df) == \"CCS\"\n",
    "        and dedup_ccs_train_labeled_cols\n",
    "    ):\n",
    "        df = compute_mean_train_test_acc_df_over_nonunique_cols(df)\n",
    "\n",
    "    get_acc_stats_for_df(df, print=True)\n",
    "    heatmap_kwargs = {\"vmin\": 0.5, \"vmax\": 1}\n",
    "    plot_heatmap(df, \"accuracy\", std_annot=False, heatmap_kwargs=heatmap_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_names_and_dfs = list(experiment_dfs.items())\n",
    "exp1, df1 = exp_names_and_dfs[-1]\n",
    "df1_method = df1.method.unique()\n",
    "if len(df1_method) > 1:\n",
    "    raise ValueError(f\"Expected one unique method, got {df1_method}\")\n",
    "train_col1 = \"train_labeled\" if method_uses_labeled_and_unlabeled_data(df1) else \"train\"\n",
    "\n",
    "for exp2, df2 in exp_names_and_dfs:\n",
    "    df2_method = df2.method.unique()\n",
    "    if len(df2_method) > 1:\n",
    "        raise ValueError(f\"Expected one unique method, got {df2_method}\")\n",
    "\n",
    "    train_col2 = (\n",
    "        \"train_labeled\" if method_uses_labeled_and_unlabeled_data(df2) else \"train\"\n",
    "    )\n",
    "    comparison = compare_accuracies(\n",
    "        df1,\n",
    "        df2,\n",
    "        train_col1,\n",
    "        train_col2,\n",
    "        is_ccs_lr_in_span_oracle=is_ccs_lr_in_span_oracle,\n",
    "    )\n",
    "    print(f\"Comparison between {exp1} and {exp2}:\")\n",
    "    pprint.pprint(comparison)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy violin plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for exp_name, df in experiment_dfs.items():\n",
    "    print(exp_name)\n",
    "    plot_violin(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "path = \"/scratch/users/ebronstein/Exhaustive-CCS/extraction_results/Llama-2-7b-chat-hf_normal-bananashed_CCS+LR_sweep/sup_weight=3-unsup_weight=1-lr=1e-2-n_epochs=1000/meta-llama-Llama-2-7b-chat-hf/nolabel_dbpedia-14-label_imdb/seed_0/1/train/fit_result_CCS+LR.json\"\n",
    "\n",
    "with open(path, \"r\") as f:\n",
    "    fit_result = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df = plot_utils.plot_accuracy(fit_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_utils.plot_history(fit_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_accuracy(fit_result, save_path=None):\n",
    "    eval_histories = fit_result[\"eval_histories\"]\n",
    "    data = []\n",
    "\n",
    "    for trial, history in enumerate(eval_histories):\n",
    "        epochs = history[\"epoch\"]\n",
    "        for split in [\"train\", \"test\"]:\n",
    "            for acc_dataset in [\"sup_acc\", \"unsup_acc\"]:\n",
    "                acc_name = f\"{split}_{acc_dataset}\"\n",
    "                acc_list = history[acc_name]\n",
    "                for epoch, acc_val in zip(epochs, acc_list):\n",
    "                    data.append(\n",
    "                        {\n",
    "                            \"trial\": trial,\n",
    "                            \"split\": split,\n",
    "                            \"type\": acc_dataset,\n",
    "                            \"epoch\": epoch,\n",
    "                            \"accuracy\": acc_val,\n",
    "                        }\n",
    "                    )\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Plot\n",
    "    fig, axs = plt.subplots(2, 1, figsize=(8, 12))\n",
    "    for ax, acc_dataset in zip(axs, [\"sup_acc\", \"unsup_acc\"]):\n",
    "        sns.lineplot(\n",
    "            data=df,\n",
    "            x=\"epoch\",\n",
    "            y=\"accuracy\",\n",
    "            style=\"split\",\n",
    "            ci=\"sd\",\n",
    "            markers=False,\n",
    "            dashes=True,\n",
    "            ax=ax,\n",
    "        )\n",
    "        ax.set_xlabel(\"Epoch\")\n",
    "        ax.set_ylabel(acc_dataset)\n",
    "\n",
    "    fig.suptitle(\"Accuracy on Train and Test Splits\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_path is None:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.savefig(save_path)\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "plot_accuracy(fit_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(\n",
    "    fit_result, vars_to_plot=None, one_fig=False, save_path=None, logger=None\n",
    "):\n",
    "    if not fit_result[\"train_histories\"] and logger is not None:\n",
    "        logger.info(\"No train_histories found in fit_result\")\n",
    "\n",
    "    if vars_to_plot is None:\n",
    "        vars_to_plot = fit_result[\"train_histories\"][0].keys()\n",
    "    else:\n",
    "        if not all(\n",
    "            var_name in fit_result[\"train_histories\"][0] for var_name in vars_to_plot\n",
    "        ):\n",
    "            raise ValueError(\n",
    "                f\"vars_to_plot contains unknown variable names: {vars_to_plot}\"\n",
    "            )\n",
    "\n",
    "    # Prepare the data for plotting\n",
    "    data = []\n",
    "    for var_name in vars_to_plot:\n",
    "        for split, histories in zip(\n",
    "            [\"train\", \"test\"],\n",
    "            [fit_result[\"train_histories\"], fit_result[\"eval_histories\"]],\n",
    "        ):\n",
    "            for trial, history in enumerate(histories):\n",
    "                var_history = history.get(var_name, [])\n",
    "                if \"epoch\" in history:\n",
    "                    epochs = history[\"epoch\"]\n",
    "                else:\n",
    "                    epochs = range(len(var_history))\n",
    "                for epoch, value in zip(epochs, var_history):\n",
    "                    data.append(\n",
    "                        {\n",
    "                            \"Epoch\": epoch,\n",
    "                            \"Value\": value,\n",
    "                            \"Type\": var_name,\n",
    "                            \"Trial\": trial,\n",
    "                            \"Split\": split,\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Plot\n",
    "    nrows = 1 if one_fig else len(vars_to_plot)\n",
    "    fig, axs = plt.subplots(nrows, 1, figsize=(8, nrows * 6))\n",
    "\n",
    "    if one_fig:\n",
    "        sns.lineplot(\n",
    "            data=df,\n",
    "            x=\"Epoch\",\n",
    "            y=\"Value\",\n",
    "            hue=\"Type\",\n",
    "            style=\"Split\",\n",
    "            ci=\"sd\",\n",
    "            markers=False,\n",
    "            dashes=True,\n",
    "            ax=axs,\n",
    "        )\n",
    "        axs.set_title(\"History with Confidence Interval\")\n",
    "        axs.set_xlabel(\"Epoch\")\n",
    "        axs.legend(title=\"Value\")\n",
    "    else:\n",
    "        if nrows == 1:\n",
    "            axs = [axs]\n",
    "\n",
    "        for ax, var_name in zip(axs, vars_to_plot):\n",
    "            sns.lineplot(\n",
    "                data=df[df[\"Type\"] == var_name],\n",
    "                x=\"Epoch\",\n",
    "                y=\"Value\",\n",
    "                style=\"Split\",\n",
    "                ci=\"sd\",\n",
    "                markers=False,\n",
    "                dashes=True,\n",
    "                ax=ax,\n",
    "            )\n",
    "            ax.set_title(f\"{var_name} History\")\n",
    "            ax.set_xlabel(\"Epoch\")\n",
    "            ax.set_ylabel(var_name)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_path is None:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.savefig(save_path)\n",
    "        plt.close()\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "plot_df = plot_history(\n",
    "    fit_result,\n",
    "    # vars_to_plot=[\"total_loss\", \"supervised_loss\"],\n",
    "    one_fig=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df.query(\"Type == 'total_loss' and Split == 'test'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(\n",
    "    data=plot_df.query(\"Type == 'total_loss'\"),\n",
    "    x=\"Epoch\",\n",
    "    y=\"Value\",\n",
    "    hue=\"Type\",\n",
    "    style=\"Split\",\n",
    "    ci=\"sd\",\n",
    "    markers=False,\n",
    "    dashes=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(\n",
    "    data=plot_df,\n",
    "    x=\"Epoch\",\n",
    "    y=\"Value\",\n",
    "    hue=\"Type\",\n",
    "    style=\"Split\",\n",
    "    ci=\"sd\",\n",
    "    markers=False,\n",
    "    dashes=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_utils.plot_accuracy(fit_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_utils.plot_history(fit_result, one_fig=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_ece = df.ece.min()\n",
    "max_ece = df.ece.max()\n",
    "heatmap_kwargs = {\"vmin\": min_ece, \"vmax\": max_ece}\n",
    "plot_heatmap(df, \"ece\", std_annot=False, heatmap_kwargs=heatmap_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calibration curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = \"extraction_results\"\n",
    "experiment_name = \"Llama-2-7b-chat-hf_v3\"\n",
    "ece_df = make_ece_df(save_dir, experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ece_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_ece_df = ece_df.query(\n",
    "    \"method == 'CCS' and train == test and seed == 1 and prompt == 0\"\n",
    ")\n",
    "for key, group_df in sample_ece_df.groupby(\n",
    "    [\"model\", \"method\", \"train\", \"test\", \"prompt\", \"seed\"]\n",
    "):\n",
    "    assert len(group_df) == 2\n",
    "    og_row = group_df[~group_df[\"flip_probs\"]].iloc[0]\n",
    "    flip_row = group_df[group_df[\"flip_probs\"]].iloc[0]\n",
    "\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "    og_title = f\"ECE={og_row.ece:.4f}\"\n",
    "    calibration_plot_from_bin_xy(\n",
    "        og_row.bin_mean_probs, og_row.bin_mean_labels, og_title, ax=ax[0]\n",
    "    )\n",
    "\n",
    "    flip_title = f\"ECE={flip_row.ece:.4f}\"\n",
    "    calibration_plot_from_bin_xy(\n",
    "        flip_row.bin_mean_probs, flip_row.bin_mean_labels, flip_title, ax=ax[1]\n",
    "    )\n",
    "\n",
    "    title = f\"{og_row.model} - {og_row.method} - train={og_row.train} - test={og_row.test} - prompt={og_row.prompt} - seed={og_row.seed}\"\n",
    "    fig.suptitle(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LR direction similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute inner products of all pairwise orthogonal directions across all\n",
    "# datasets.\n",
    "\n",
    "exp_dir = Path(\n",
    "    \"/nas/ucb/ebronstein/Exhaustive-CCS/extraction_results/Llama-2-7b-chat-hf_normal-bananashed_CCS-in-LR-span-convex_20_orth_dirs/meta-llama-Llama-2-7b-chat-hf\"\n",
    ")\n",
    "# Arbitrarily use IMDB as the unlabeled/unsupervised dataset to be consistent\n",
    "# and because we only want to change the labeled dataset, which is used to\n",
    "# train the orthogonal LR directions. Probably due to randomness, the LR\n",
    "# directions are not the same across different unlabeled datasets for a given\n",
    "# labeled dataset.\n",
    "paths = exp_dir.glob(\"nolabel_imdb*/**/orthogonal_directions.npy\")\n",
    "inner_prods_df = make_inner_prods_df(paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First direction only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_inner_prods_heatmap(inner_prods_df.query(\"dir1_idx == dir2_idx == 0\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most similar directions for each dataset pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the most similar directions by taking the max of the inner products for\n",
    "# each dataset pair.\n",
    "max_inner_prods_per_ds_combo_df = (\n",
    "    inner_prods_df.groupby([\"ds1\", \"ds2\"])\n",
    "    .inner_prod.max()\n",
    "    .unstack()\n",
    "    .reindex(index=DATASET_ORDER, columns=DATASET_ORDER)\n",
    ")\n",
    "plot_inner_prods_heatmap(max_inner_prods_per_ds_combo_df, needs_pivot=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top similar directions for each dataset pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ds1, ds2 in itertools.combinations(DATASET_ORDER, r=2):\n",
    "    display(\n",
    "        inner_prods_df.query(\"ds1 == @ds1 and ds2 == @ds2\")\n",
    "        .sort_values(\"inner_prod\", ascending=False)\n",
    "        .head(5)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity of random vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "dims = list(range(1, 1000, 100))\n",
    "matrices = [np.random.normal(size=(1000, N)) for N in dims]\n",
    "res = [np.abs(cosine_similarity(matrix, matrix)).mean() for matrix in matrices]\n",
    "plt.plot(dims, res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecs = np.random.normal(size=(10000, 4096))\n",
    "similarities = cosine_similarity(vecs, vecs)\n",
    "mean_abs_sim = np.abs(similarities).mean()\n",
    "mean_abs_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LR in span analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute inner products of all pairwise orthogonal directions across all\n",
    "# datasets.\n",
    "\n",
    "exp_dir = Path(\n",
    "    \"/nas/ucb/ebronstein/Exhaustive-CCS/extraction_results/Llama-2-7b-chat-hf_normal-bananashed_CCS-in-LR-span-convex_20_orth_dirs/meta-llama-Llama-2-7b-chat-hf\"\n",
    ")\n",
    "# Arbitrarily use IMDB as the unlabeled/unsupervised dataset to be consistent\n",
    "# and because we only want to change the labeled dataset, which is used to\n",
    "# train the orthogonal LR directions. Probably due to randomness, the LR\n",
    "# directions are not the same across different unlabeled datasets for a given\n",
    "# labeled dataset.\n",
    "paths = exp_dir.glob(\"nolabel_imdb*/**/orthogonal_directions.npy\")\n",
    "train_to_orth_dirs = get_train_to_orth_dirs_dict(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orth_dirs_1 = train_to_orth_dirs[\"imdb\"]\n",
    "orth_dirs_2 = train_to_orth_dirs[\"amazon-polarity\"]\n",
    "\n",
    "rows = []\n",
    "for ds1, ds2 in itertools.product(train_to_orth_dirs, repeat=2):\n",
    "    orth_dirs_1 = train_to_orth_dirs[ds1]\n",
    "    orth_dirs_2 = train_to_orth_dirs[ds2]\n",
    "\n",
    "    # Project each direction in orth_dirs_1 onto each direction in orth_dirs_2.\n",
    "    # inner_prods[i, j] is the inner product of the i-th direction in orth_dirs_1\n",
    "    # with the j-th direction in orth_dirs_2.\n",
    "    inner_prods = orth_dirs_1 @ orth_dirs_2.T\n",
    "\n",
    "    # orth_dirs_1 projected onto orth_dirs_2. dirs_1_proj_dirs2[i] is the i-th\n",
    "    # direction from orth_dirs_1 projected onto the span of the rows of orth_dirs_2.\n",
    "    dirs_1_proj_dirs_2 = orth_dirs_1 @ orth_dirs_2.T @ orth_dirs_2  # [n_dirs, hidden_dim]\n",
    "\n",
    "    # Fraction (norm-wise) that each direction in orth_dirs_1 is in the span of the\n",
    "    # directions in orth_dirs_2. Shape: [n_dirs].\n",
    "    # NOTE: the division by the norm of each orthogonal direction should not be\n",
    "    # necessary since the orthogonal directions should be normalized.\n",
    "    frac_dirs_1_in_span_dirs_2 = np.linalg.norm(dirs_1_proj_dirs_2, axis=1) / np.linalg.norm(\n",
    "        orth_dirs_1, axis=1\n",
    "    )\n",
    "    for i in range(frac_dirs_1_in_span_dirs_2.shape[0]):\n",
    "        rows.append({\n",
    "            \"ds1\": ds1,\n",
    "            \"ds2\": ds2,\n",
    "            \"dir1_idx\": i,\n",
    "            \"frac_in_span\": frac_dirs_1_in_span_dirs_2[i],\n",
    "        })\n",
    "\n",
    "frac_in_span_df = pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frac_in_span_df.query(\"dir1_idx == 0\").pivot(\n",
    "    index=\"ds1\", columns=\"ds2\", values=\"frac_in_span\"\n",
    ").reindex(index=DATASET_ORDER, columns=DATASET_ORDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First direction only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_heatmap_from_df(\n",
    "    frac_in_span_df.query(\"dir1_idx == 0\"),\n",
    "    needs_pivot=True,\n",
    "    pivot_values=\"frac_in_span\",\n",
    "    cbar_label=\"Fraction in Span\",\n",
    "    title=\"Frac. Set 2's Best LR Direction in Span of Set 1's LR Directions\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most in-span direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_frac_in_span_per_ds_combo_df = (\n",
    "    frac_in_span_df.groupby([\"ds1\", \"ds2\"])[\"frac_in_span\"]\n",
    "    .max()\n",
    "    .unstack()\n",
    "    .reindex(index=DATASET_ORDER, columns=DATASET_ORDER)\n",
    ")\n",
    "plot_heatmap_from_df(\n",
    "    max_frac_in_span_per_ds_combo_df,\n",
    "    needs_pivot=False,\n",
    "    cbar_label=\"Max Frac. in Span\",\n",
    "    title=\"Max Frac. Set 2's LR Directions in Span of Set 1's LR Directions\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top in-span direction for each dataset combo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ds1, ds2 in itertools.combinations(DATASET_ORDER, r=2):\n",
    "    display(\n",
    "        frac_in_span_df.query(\"ds1 == @ds1 and ds2 == @ds2\")\n",
    "        .sort_values(\"frac_in_span\", ascending=False)\n",
    "        .head(5)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In-span-ness of random vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gs(X, row_vecs=True, norm=True):\n",
    "    \"\"\"Gram-Schmidt process for orthonormalizing a set of vectors.\"\"\"\n",
    "    if not row_vecs:\n",
    "        X = X.T\n",
    "    Y = X[0:1, :].copy()\n",
    "    for i in range(1, X.shape[0]):\n",
    "        proj = np.diag((X[i, :].dot(Y.T) / np.linalg.norm(Y, axis=1) ** 2).flat).dot(Y)\n",
    "        Y = np.vstack((Y, X[i, :] - proj.sum(0)))\n",
    "    if norm:\n",
    "        Y = np.diag(1 / np.linalg.norm(Y, axis=1)).dot(Y)\n",
    "    if row_vecs:\n",
    "        return Y\n",
    "    else:\n",
    "        return Y.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs_1 = np.random.normal(size=(100, 20, 4096))\n",
    "dirs_2 = np.random.normal(size=(100, 20, 4096))\n",
    "\n",
    "# Normalize the directions\n",
    "dirs_1 = dirs_1 / np.linalg.norm(dirs_1, axis=-1, keepdims=True)\n",
    "dirs_2 = dirs_2 / np.linalg.norm(dirs_2, axis=-1, keepdims=True)\n",
    "\n",
    "# Make an orthonormal basis for the span of the rows of each matrix\n",
    "orth_dirs_1 = np.stack([gs(x) for x in dirs_1])\n",
    "orth_dirs_2 = np.stack([gs(x) for x in dirs_2])\n",
    "\n",
    "inner_prods = orth_dirs_1 @ orth_dirs_2.transpose(0, 2, 1)\n",
    "\n",
    "# orth_dirs_1 projected onto orth_dirs_2. dirs_1_proj_dirs2[i] is the i-th\n",
    "# direction from orth_dirs_1 projected onto the span of the rows of orth_dirs_2.\n",
    "dirs_1_proj_dirs_2 = (\n",
    "    orth_dirs_1 @ orth_dirs_2.transpose(0, 2, 1) @ orth_dirs_2\n",
    ")  # [n_dirs, hidden_dim]\n",
    "\n",
    "# Fraction (norm-wise) that each direction in orth_dirs_1 is in the span of the\n",
    "# directions in orth_dirs_2. Shape: [n_dirs].\n",
    "frac_dirs_1_in_span_dirs_2 = np.linalg.norm(dirs_1_proj_dirs_2, axis=-1)\n",
    "\n",
    "frac_dirs_1_in_span_dirs_2.max(axis=-1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frac_dirs_1_in_span_dirs_2.max(axis=-1).std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Oracle LR in LR span"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_orthogonal_directions_dataset(load_orthogonal_directions_dir: str) -> str:\n",
    "    \"\"\"Return the dataset for which the orthogonal directions were computed.\"\"\"\n",
    "    combined_datasets_str = os.path.basename(\n",
    "        os.path.normpath(load_orthogonal_directions_dir)\n",
    "    )\n",
    "    return parse_combined_datasets(combined_datasets_str)[1]\n",
    "\n",
    "\n",
    "df = experiment_dfs[\n",
    "    \"Llama-2-7b-chat-hf_normal-bananashed_oracle-LR-in-LR-span-convex_4_orth_dirs\"\n",
    "]\n",
    "\n",
    "df = df[df[\"train\"] == df[\"train_labeled\"]]\n",
    "df = df[df[\"train\"] == df[\"test\"]]\n",
    "df = df.copy()\n",
    "df[\"train\"] = df.load_orthogonal_directions_dir.apply(get_orthogonal_directions_dataset)\n",
    "df[\"train_labeled\"] = df[\"train\"]\n",
    "\n",
    "heatmap_kwargs = {\"vmin\": 0.5, \"vmax\": 1}\n",
    "plot_heatmap(df, \"accuracy\", std_annot=False, heatmap_kwargs=heatmap_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Span weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_dir = Path(\"/nas/ucb/ebronstein/Exhaustive-CCS/extraction_results/Llama-2-7b-chat-hf_normal-bananashed_oracle-LR-in-LR-span-convex_15_orth_dirs/meta-llama-Llama-2-7b-chat-hf\")\n",
    "\n",
    "paths = exp_dir.glob(\"**/coef.npy\")\n",
    "rows = []\n",
    "for path in paths:\n",
    "    coef = np.load(path).squeeze(0)\n",
    "    weights = np.exp(coef)\n",
    "    weights /= weights.sum()\n",
    "\n",
    "    with open(Path(*path.parts[:-3]) / \"config.json\", \"r\") as f:\n",
    "        config = json.load(f)\n",
    "\n",
    "    train_ds = config[\"labeled_datasets\"]\n",
    "    test_ds = parse_combined_datasets(Path(config[\"load_orthogonal_directions_dir\"]).name)[1]\n",
    "\n",
    "    rows.append({\n",
    "        \"train\": train_ds,\n",
    "        \"test\": test_ds,\n",
    "        \"weights\": weights,\n",
    "    })\n",
    "\n",
    "weights_df = pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_long = weights_df.explode(\"weights\")\n",
    "df_long[\"weights\"] = df_long[\"weights\"].astype(float)\n",
    "df_long[\"weight_id\"] = df_long.groupby([\"train\", \"test\"]).cumcount()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the FacetGrid\n",
    "g = sns.FacetGrid(\n",
    "    df_long,\n",
    "    row=\"train\",\n",
    "    col=\"test\",\n",
    "    row_order=DATASET_ORDER,\n",
    "    col_order=DATASET_ORDER,\n",
    "    margin_titles=True,\n",
    "    sharey=False,\n",
    ")\n",
    "\n",
    "# Mapping the function to the grid\n",
    "g.map(plt.hist, \"weights\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_stacked_bar(data, **kwargs):\n",
    "    print(data.train.unique(), data.test.unique())\n",
    "\n",
    "g.map_dataframe(debug_stacked_bar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Oracle LR in CCS span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unsup_orthogonal_directions_dataset(load_orthogonal_directions_dir: str) -> str:\n",
    "    \"\"\"Return the dataset for which the unsupervised orthogonal directions were computed.\"\"\"\n",
    "    combined_datasets_str = os.path.basename(\n",
    "        os.path.normpath(load_orthogonal_directions_dir)\n",
    "    )\n",
    "    return parse_combined_datasets(combined_datasets_str)[0]\n",
    "\n",
    "\n",
    "# Setup: train = train_labeled = test is when we trained an oracle LR using the\n",
    "# labeled test set. The actual \"train\" is the dataset we used to compute the\n",
    "# orthogonal CCS directions, which we extraction from the\n",
    "# load_orthogonal_directions_dir.\n",
    "\n",
    "for experiment_name in [\n",
    "    # \"Llama-2-7b-chat-hf_normal-bananashed_LR-in-CCS-span-convex_1_orth_dirs-v2\",\n",
    "    # \"Llama-2-7b-chat-hf_normal-bananashed_LR-in-CCS-span-convex_2_orth_dirs-v2\",\n",
    "    # \"Llama-2-7b-chat-hf_normal-bananashed_LR-in-CCS-span-convex_4_orth_dirs-v2\",\n",
    "    # \"Llama-2-7b-chat-hf_normal-bananashed_LR-in-CCS-span-convex_8_orth_dirs-v2\",\n",
    "    # \"Llama-2-7b-chat-hf_normal-bananashed_LR-in-CCS-span-convex_15_orth_dirs-v2\",\n",
    "    \"Llama-2-7b-chat-hf_normal-bananashed_LR-in-CCS-span-convex_20_orth_dirs\",\n",
    "    \"Llama-2-7b-chat-hf_normal-bananashed_LR-in-CCS-span-convex_100_orth_dirs\",\n",
    "]:\n",
    "    print(experiment_name)\n",
    "    df = experiment_dfs[experiment_name]\n",
    "\n",
    "    # Since the unlabeled training datasets were not used to train the oracle LR,\n",
    "    # we set train and train_labeled to be the same. Assert that this is the case.\n",
    "    assert df[\"train\"].equals(df[\"train_labeled\"])\n",
    "    # Dataset that was used to get the CCS orthogonal directions.\n",
    "    df[\"orth_dirs_ds\"] = df.load_orthogonal_directions_dir.apply(\n",
    "        get_unsup_orthogonal_directions_dataset\n",
    "    )\n",
    "\n",
    "    # Filter train == test since we're using an oracle LR.\n",
    "    df = df.query(\"train == test\")\n",
    "    df = df.copy()\n",
    "    # Set the \"train\" dataset to be the dataset for which the\n",
    "    # orthogonal directions were computed so that we can reuse the\n",
    "    # plot_heatmap function, which by default plots train vs. test.\n",
    "    df[\"train\"] = df[\"orth_dirs_ds\"]\n",
    "    df[\"train_labeled\"] = df[\"orth_dirs_ds\"]\n",
    "\n",
    "    num_orth_dirs = df.num_orthogonal_directions.unique()\n",
    "    if len(num_orth_dirs) > 1:\n",
    "        raise ValueError(\n",
    "            f\"Expected one unique num_orthogonal_directions, got {num_orth_dirs}\"\n",
    "        )\n",
    "    num_orth_dirs = num_orth_dirs[0]\n",
    "\n",
    "    heatmap_kwargs = {\"vmin\": 0.0, \"vmax\": 1}\n",
    "    plot_heatmap(\n",
    "        df,\n",
    "        \"accuracy\",\n",
    "        std_annot=False,\n",
    "        heatmap_kwargs=heatmap_kwargs,\n",
    "        xlabel=\"Train/Test Dataset\",\n",
    "        ylabel=\"Orthogonal Directions Dataset\",\n",
    "        title=f\"Llama-2-7b-chat-hf - Oracle LR in CCS Span ({num_orth_dirs}) - Mean Acc.\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LR in CCS span"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unsup_orthogonal_directions_dataset(load_orthogonal_directions_dir: str) -> str:\n",
    "    \"\"\"Return the dataset for which the unsupervised orthogonal directions were computed.\"\"\"\n",
    "    combined_datasets_str = os.path.basename(\n",
    "        os.path.normpath(load_orthogonal_directions_dir)\n",
    "    )\n",
    "    return parse_combined_datasets(combined_datasets_str)[0]\n",
    "\n",
    "\n",
    "# Setup: train = train_labeled = test is when we trained an oracle LR using the\n",
    "# labeled test set. The actual \"train\" is the dataset we used to compute the\n",
    "# orthogonal CCS directions, which we extraction from the\n",
    "# load_orthogonal_directions_dir.\n",
    "\n",
    "for experiment_name in [\n",
    "    \"Llama-2-7b-chat-hf_normal-bananashed_LR-in-CCS-span-convex_1_orth_dirs-v2\",\n",
    "    \"Llama-2-7b-chat-hf_normal-bananashed_LR-in-CCS-span-convex_2_orth_dirs-v2\",\n",
    "    \"Llama-2-7b-chat-hf_normal-bananashed_LR-in-CCS-span-convex_4_orth_dirs-v2\",\n",
    "    \"Llama-2-7b-chat-hf_normal-bananashed_LR-in-CCS-span-convex_8_orth_dirs-v2\",\n",
    "    \"Llama-2-7b-chat-hf_normal-bananashed_LR-in-CCS-span-convex_15_orth_dirs-v2\",\n",
    "    \"Llama-2-7b-chat-hf_normal-bananashed_LR-in-CCS-span-convex_20_orth_dirs\",\n",
    "    \"Llama-2-7b-chat-hf_normal-bananashed_LR-in-CCS-span-convex_100_orth_dirs\",\n",
    "]:\n",
    "    print(experiment_name)\n",
    "    df = experiment_dfs[experiment_name]\n",
    "\n",
    "    # Since the unlabeled training datasets were not used to train the oracle LR,\n",
    "    # we set train and train_labeled to be the same. Assert that this is the case.\n",
    "    assert df[\"train\"].equals(df[\"train_labeled\"])\n",
    "    # Dataset that was used to get the CCS orthogonal directions.\n",
    "    df[\"orth_dirs_ds\"] = df.load_orthogonal_directions_dir.apply(\n",
    "        get_unsup_orthogonal_directions_dataset\n",
    "    )\n",
    "    # Filter the DataFrame where the test dataset is the same as the dataset for\n",
    "    # which the orthogonal directions were computed.\n",
    "    df = df[df[\"test\"] == df[\"orth_dirs_ds\"]]\n",
    "\n",
    "    num_orth_dirs = df.num_orthogonal_directions.unique()\n",
    "    if len(num_orth_dirs) > 1:\n",
    "        raise ValueError(\n",
    "            f\"Expected one unique num_orthogonal_directions, got {num_orth_dirs}\"\n",
    "        )\n",
    "    num_orth_dirs = num_orth_dirs[0]\n",
    "\n",
    "    heatmap_kwargs = {\"vmin\": 0.0, \"vmax\": 1}\n",
    "    plot_heatmap(\n",
    "        df,\n",
    "        \"accuracy\",\n",
    "        std_annot=False,\n",
    "        heatmap_kwargs=heatmap_kwargs,\n",
    "        xlabel=\"Test (Orthogonal Directions) Dataset\",\n",
    "        ylabel=\"Train Dataset\",\n",
    "        title=f\"Llama-2-7b-chat-hf - LR in CCS Span ({num_orth_dirs}) - Mean Acc.\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example filters (set to None if you don't want to filter by these)\n",
    "model_filter = None\n",
    "train_set_filter = None\n",
    "eval_set_filter = None\n",
    "use_latest_run_id = True\n",
    "use_best_in_domain_acc = True\n",
    "is_ccs_lr_in_span_oracle = False\n",
    "\n",
    "experiment_dfs = load_eval_csvs(\n",
    "    \"extraction_results/mistralai/Mistral-7B-Instruct-v0.2/normal/layer_-1/ccs_lr_in_span/mode_concat/sup_weight_1/unsup_weight_0/lr_0.01/n_epochs_5000\",\n",
    "    pattern=None,\n",
    "    model_filter=model_filter,\n",
    "    train_set_filter=train_set_filter,\n",
    "    eval_set_filter=eval_set_filter,\n",
    "    use_latest_run_id=use_latest_run_id,\n",
    "    use_best_in_domain_acc=use_best_in_domain_acc,\n",
    "    is_ccs_lr_in_span_oracle=is_ccs_lr_in_span_oracle,\n",
    ")\n",
    "\n",
    "# Exclude df with 100 orthogonal directions because that was just used to make\n",
    "# the span dirs.\n",
    "experiment_dfs = {\n",
    "    key: df for key, df in experiment_dfs.items() if \"n_orth_dirs_100\" not in key\n",
    "}\n",
    "experiment_dfs = {\n",
    "    key: get_lr_in_ccs_span_transfer_df(df) for key, df in experiment_dfs.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unsup_orthogonal_directions_dataset(load_orthogonal_directions_dir: str) -> str:\n",
    "    \"\"\"Return the dataset for which the unsupervised orthogonal directions were computed.\"\"\"\n",
    "    combined_datasets_str = os.path.basename(\n",
    "        os.path.normpath(load_orthogonal_directions_dir)\n",
    "    )\n",
    "    return parse_combined_datasets(combined_datasets_str)[0]\n",
    "\n",
    "\n",
    "def get_lr_in_ccs_span_transfer_df(df):\n",
    "    # Since the unlabeled training datasets were not used to train the oracle LR,\n",
    "    # we set train and train_labeled to be the same. Assert that this is the case.\n",
    "    assert df[\"train\"].equals(df[\"train_labeled\"])\n",
    "    # Dataset that was used to get the CCS orthogonal directions.\n",
    "    df[\"orth_dirs_ds\"] = df.load_orthogonal_directions_dir.apply(\n",
    "        get_unsup_orthogonal_directions_dataset\n",
    "    )\n",
    "    # Filter the DataFrame where the test dataset is the same as the dataset for\n",
    "    # which the orthogonal directions were computed.\n",
    "    df = df[df[\"test\"] == df[\"orth_dirs_ds\"]]\n",
    "\n",
    "    num_orth_dirs = df.num_orthogonal_directions.unique()\n",
    "    if len(num_orth_dirs) > 1:\n",
    "        raise ValueError(\n",
    "            f\"Expected one unique num_orthogonal_directions, got {num_orth_dirs}\"\n",
    "        )\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "num_orth_dirs_list = []\n",
    "for p, df in experiment_dfs.items():\n",
    "    num_orth_dirs = int(p.split(\"/n_orth_dirs_\")[1].split(\"/\")[0])\n",
    "    num_orth_dirs_list.append(num_orth_dirs)\n",
    "\n",
    "num_orth_dirs_exp_df_items = sorted(\n",
    "    zip(num_orth_dirs_list, experiment_dfs.items()), key=lambda x: x[0]\n",
    ")\n",
    "dfs = [df for _, (_, df) in num_orth_dirs_exp_df_items]\n",
    "titles = [\n",
    "    f\"num_orth_dirs {num_orth_dirs}\"\n",
    "    for num_orth_dirs, (_, df) in num_orth_dirs_exp_df_items\n",
    "]\n",
    "\n",
    "print(\"Experiments:\")\n",
    "for _, (path, _) in num_orth_dirs_exp_df_items:\n",
    "    print(path)\n",
    "\n",
    "filter_train_equals_test = False\n",
    "# If we accidentally evaluated CCS on multiple train_labeled columns per\n",
    "# (train, test) pair, take the mean accuracy over the train_labeled column per\n",
    "# pair, which should be identical (up to randomness in the training).\n",
    "dedup_ccs_train_labeled_cols = True\n",
    "\n",
    "heatmap_subplots(\n",
    "    dfs,\n",
    "    titles,\n",
    "    filter_train_equals_test=filter_train_equals_test,\n",
    "    dedup_ccs_train_labeled_cols=dedup_ccs_train_labeled_cols,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spot check the transfer accuracy.\n",
    "df = dfs[0].query(\"train_labeled == 'imdb'\")\n",
    "rows = []\n",
    "for key, group_df in df.groupby([\"train\", \"train_labeled\", \"orth_dirs_ds\", \"test\"]):\n",
    "    accuracy = group_df[\"accuracy\"].mean()\n",
    "    load_orthogonal_directions_dir = group_df[\"load_orthogonal_directions_dir\"].unique()\n",
    "    assert len(load_orthogonal_directions_dir) == 1\n",
    "    load_orthogonal_directions_dir = load_orthogonal_directions_dir[0]\n",
    "    rows.append(\n",
    "        {\n",
    "            \"train\": key[0],\n",
    "            \"train_labeled\": key[1],\n",
    "            \"orth_dirs_ds\": key[2],\n",
    "            \"load_orthogonal_directions_dir\": load_orthogonal_directions_dir,\n",
    "            \"test\": key[3],\n",
    "            \"accuracy\": accuracy,\n",
    "        }\n",
    "    )\n",
    "df = pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the orth_dirs_ds is correct based on the directory orth dirs are\n",
    "# loaded from.\n",
    "for i in range(len(df)):\n",
    "    print(df.iloc[i].load_orthogonal_directions_dir, df.iloc[i].orth_dirs_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Span weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_dir = Path(\"/nas/ucb/ebronstein/Exhaustive-CCS/extraction_results/Llama-2-7b-chat-hf_normal-bananashed_LR-in-CCS-span-convex_20_orth_dirs-v2\")\n",
    "\n",
    "paths = exp_dir.glob(\"**/coef.npy\")\n",
    "rows = []\n",
    "for path in paths:\n",
    "    coef = np.load(path).squeeze(0)\n",
    "    weights = np.exp(coef)\n",
    "    weights /= weights.sum()\n",
    "\n",
    "    with open(Path(*path.parts[:-3]) / \"config.json\", \"r\") as f:\n",
    "        config = json.load(f)\n",
    "\n",
    "    train_ds = config[\"datasets\"]\n",
    "    test_ds = parse_combined_datasets(Path(config[\"load_orthogonal_directions_dir\"]).name)[0]\n",
    "\n",
    "    rows.append({\n",
    "        \"train\": train_ds,\n",
    "        \"test\": test_ds,\n",
    "        \"weights\": weights,\n",
    "    })\n",
    "\n",
    "weights_df = pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_long = weights_df.explode(\"weights\")\n",
    "df_long[\"weights\"] = df_long[\"weights\"].astype(float)\n",
    "df_long[\"weight_id\"] = df_long.groupby([\"train\", \"test\"]).cumcount()\n",
    "\n",
    "# Setting up the FacetGrid\n",
    "g = sns.FacetGrid(\n",
    "    df_long,\n",
    "    row=\"train\",\n",
    "    col=\"test\",\n",
    "    row_order=DATASET_ORDER,\n",
    "    col_order=DATASET_ORDER,\n",
    "    margin_titles=True,\n",
    "    sharey=False,\n",
    ")\n",
    "\n",
    "# Mapping the function to the grid\n",
    "g.map(plt.hist, \"weights\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter search/analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CCS+LR: lr, sup_weight, unsup_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = experiment_dfs[\"Llama-2-7b-chat-hf_normal-bananashed_CCS+LR_sweep\"]\n",
    "df = experiment_dfs[\"Llama-2-7b-chat-hf_normal-bananashed_CCS+LR_adam\"]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs = {}\n",
    "for key, group_df in df.groupby(\n",
    "    [\n",
    "        \"model\",\n",
    "        \"method\",\n",
    "        \"prefix\",\n",
    "        \"mode\",\n",
    "        \"train\",\n",
    "        \"train_labeled\",\n",
    "        \"test\",\n",
    "        \"location\",\n",
    "        \"layer\",\n",
    "    ]\n",
    "):\n",
    "    accs[key] = {}\n",
    "    for hparam_key, hparam_group_df in group_df.groupby(\n",
    "        [\"sup_weight\", \"unsup_weight\", \"lr\", \"n_epochs\"]\n",
    "    ):\n",
    "        acc = hparam_group_df.accuracy.mean()\n",
    "        accs[key][hparam_key] = acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_regret_dict = defaultdict(int)\n",
    "\n",
    "for key, hparam_dict in accs.items():\n",
    "    (\n",
    "        model,\n",
    "        method,\n",
    "        prefix,\n",
    "        mode,\n",
    "        train,\n",
    "        train_labeled,\n",
    "        test,\n",
    "        location,\n",
    "        layer,\n",
    "    ) = key\n",
    "    if train != test:\n",
    "        continue\n",
    "\n",
    "    print(key)\n",
    "    best_acc = max(hparam_dict.values())\n",
    "    for hparam_key, acc in sorted(\n",
    "        hparam_dict.items(), key=lambda key_acc: key_acc[1], reverse=True\n",
    "    ):\n",
    "        print(hparam_key, acc)\n",
    "        acc_regret = best_acc - acc\n",
    "        acc_regret_dict[hparam_key] += acc_regret\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LR: C, max_iter, penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = experiment_dfs[\"Llama-2-7b-chat-hf_normal-bananashed_LR-mode=concat-sweep\"].copy()\n",
    "df[\"log_reg.penalty\"] = df[\"log_reg.penalty\"].apply(\n",
    "    lambda x: \"None\" if pd.isnull(x) else x\n",
    ")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs = {}\n",
    "for key, group_df in df.groupby(\n",
    "    [\n",
    "        \"model\",\n",
    "        \"method\",\n",
    "        \"prefix\",\n",
    "        \"mode\",\n",
    "        \"train\",\n",
    "        \"train_labeled\",\n",
    "        \"test\",\n",
    "        \"location\",\n",
    "        \"layer\",\n",
    "    ]\n",
    "):\n",
    "    accs[key] = {}\n",
    "    for hparam_key, hparam_group_df in group_df.groupby(\n",
    "        [\"log_reg.C\", \"log_reg.max_iter\", \"log_reg.penalty\"], dropna=False\n",
    "    ):\n",
    "        acc = hparam_group_df.accuracy.mean()\n",
    "        accs[key][hparam_key] = acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_regret_dict = defaultdict(int)\n",
    "\n",
    "for key, hparam_dict in accs.items():\n",
    "    (\n",
    "        model,\n",
    "        method,\n",
    "        prefix,\n",
    "        mode,\n",
    "        train,\n",
    "        train_labeled,\n",
    "        test,\n",
    "        location,\n",
    "        layer,\n",
    "    ) = key\n",
    "    print(key)\n",
    "    best_acc = max(hparam_dict.values())\n",
    "    for hparam_key, acc in sorted(\n",
    "        hparam_dict.items(), key=lambda key_acc: key_acc[1], reverse=True\n",
    "    ):\n",
    "        print(hparam_key, acc)\n",
    "        acc_regret = best_acc - acc\n",
    "        acc_regret_dict[hparam_key] += acc_regret\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(acc_regret_dict.items(), key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(\n",
    "    [\n",
    "        0.7674999833106995,\n",
    "        0.7949999570846558,\n",
    "        0.8100000023841858,\n",
    "        0.8524999618530273,\n",
    "        0.7924999594688416,\n",
    "        0.8499999642372131,\n",
    "        0.762499988079071,\n",
    "        0.7475000023841858,\n",
    "        0.8449999690055847,\n",
    "        0.7949999570846558,\n",
    "        0.7799999713897705,\n",
    "        0.7899999618530273,\n",
    "        0.7924999594688416,\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CCS in LR span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/nas/ucb/ebronstein/Exhaustive-CCS/extraction_results/Llama-2-7b-chat-hf_normal-bananashed_CCS-in-LR-span-convex_20_orth_dirs/meta-llama-Llama-2-7b-chat-hf/nolabel_imdb-label_boolq/seed_0/1/train/fit_result_CCS-in-LR-span.json\"\n",
    "with open(path, \"r\") as f:\n",
    "    fit_result = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orth_dir_acc_df = pd.DataFrame(fit_result[\"lr_fit_results\"])\n",
    "\n",
    "max_n_dirs = 200\n",
    "sup_acc_df = orth_dir_acc_df[[\"sup_train_acc\", \"sup_test_acc\"]].iloc[:max_n_dirs]\n",
    "unsup_acc_df = orth_dir_acc_df[[\"unsup_train_acc\", \"unsup_test_acc\"]].iloc[:max_n_dirs]\n",
    "\n",
    "fig, axs = plt.subplots(2, 1, figsize=(10, 6))\n",
    "\n",
    "sns.lineplot(sup_acc_df, alpha=0.5, ax=axs[0])\n",
    "axs[0].set_xlabel(\"Direction\")\n",
    "axs[0].set_ylabel(\"Accuracy\")\n",
    "# axs[0].set_title(\"Accuracy on Supervised Data\")\n",
    "\n",
    "sns.lineplot(unsup_acc_df, alpha=0.5, ax=axs[1])\n",
    "axs[1].set_xlabel(\"Direction\")\n",
    "axs[1].set_ylabel(\"Accuracy\")\n",
    "# axs[1].set_title(\"Accuracy on Supervised Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CCS select LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = \"extraction_results\"\n",
    "experiment_name = \"Llama-2-7b-chat-hf_normal-bananashed_CCS-select-LR-sweep_num_dirs\"\n",
    "df = load_eval_csvs(\n",
    "    save_dir,\n",
    "    experiment_name,\n",
    "    use_latest_run_id=False,  # Different run IDs use different numbers of directions\n",
    "    use_best_in_domain_acc=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy vs. num orthogonal directions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (model, method, train, train_labeled, test), group_df in df.groupby(\n",
    "    [\"model\", \"method\", \"train\", \"train_labeled\", \"test\"]\n",
    "):\n",
    "    for num_orthogonal_directions, num_orthogonal_group_df in group_df.groupby(\n",
    "        \"num_orthogonal_directions\"\n",
    "    ):\n",
    "        assert num_orthogonal_group_df.run_id.nunique() == 1\n",
    "        accuracy = num_orthogonal_group_df.accuracy.mean()\n",
    "        print(\n",
    "            model,\n",
    "            method,\n",
    "            train,\n",
    "            train_labeled,\n",
    "            test,\n",
    "            num_orthogonal_directions,\n",
    "            accuracy,\n",
    "        )\n",
    "\n",
    "    sns.lineplot(\n",
    "        data=group_df, x=\"num_orthogonal_directions\", y=\"accuracy\", errorbar=\"ci\"\n",
    "    )\n",
    "    plt.xscale(\"log\", base=2)\n",
    "    plt.suptitle(\n",
    "        f\"{model} | labeled train={train_labeled} | unlabeled train={train} | test={test}\"\n",
    "    )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which dir is chosen vs. num dirs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(\n",
    "    \"/nas/ucb/ebronstein/Exhaustive-CCS/extraction_results/Llama-2-7b-chat-hf_normal-bananashed_CCS-select-LR-sweep_num_dirs/meta-llama-Llama-2-7b-chat-hf/nolabel_dbpedia-14-label_imdb/seed_0\"\n",
    ")\n",
    "\n",
    "all_num_orthogonal_directions = []\n",
    "all_best_idx = []\n",
    "for run_id_dir in path.glob(\"*\"):\n",
    "    if run_id_dir.is_dir():\n",
    "        config_path = run_id_dir / \"config.json\"\n",
    "        with open(config_path, \"r\") as f:\n",
    "            config = json.load(f)\n",
    "        num_orthogonal_directions = config[\"num_orthogonal_directions\"]\n",
    "        all_num_orthogonal_directions.append(num_orthogonal_directions)\n",
    "\n",
    "        fit_result_paths = (run_id_dir / \"train\").glob(\"fit_result*.json\")\n",
    "        for path in fit_result_paths:\n",
    "            with open(path, \"r\") as f:\n",
    "                fit_result = json.load(f)\n",
    "            all_best_idx.append(fit_result[\"best_idx\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for num_dirs, best_idx in sorted(\n",
    "    zip(all_num_orthogonal_directions, all_best_idx), key=lambda x: x[0]\n",
    "):\n",
    "    print(f\"# directions: {num_dirs}, best idx: {best_idx}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CCS loss per direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CCS-select-LR result\n",
    "path = \"/nas/ucb/ebronstein/Exhaustive-CCS/extraction_results/Llama-2-7b-chat-hf_normal-bananashed_CCS-select-LR-sweep_num_dirs/meta-llama-Llama-2-7b-chat-hf/nolabel_dbpedia-14-label_imdb/seed_0/11/train/fit_result_CCS-select-LR.json\"\n",
    "with open(path, \"r\") as f:\n",
    "    fit_result = json.load(f)\n",
    "\n",
    "# LR orthogonal directions fit results\n",
    "path = \"/nas/ucb/ebronstein/Exhaustive-CCS/extraction_results/Llama-2-7b-chat-hf_normal-bananashed_CCS-in-LR-span-sweep_num_dirs/meta-llama-Llama-2-7b-chat-hf/nolabel_dbpedia-14-label_imdb/seed_0/2/train/fit_result_CCS-in-LR-span.json\"\n",
    "with open(path, \"r\") as f:\n",
    "    lr_orth_dirs_fit_result = json.load(f)\n",
    "\n",
    "lr_orth_dirs_acc_df = pd.DataFrame(lr_orth_dirs_fit_result[\"lr_fit_results\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(data=df.reset_index().melt(\"index\"), x=\"index\", y=\"value\", hue=\"variable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_idx = fit_result[\"best_idx\"]\n",
    "\n",
    "loss_to_acc_name = {\n",
    "    \"train_unlabeled_losses\": \"unsup_train_acc\",\n",
    "    \"test_unlabeled_losses\": \"unsup_test_acc\",\n",
    "    \"train_labeled_losses\": \"sup_train_acc\",\n",
    "    \"test_labeled_losses\": \"sup_test_acc\",\n",
    "}\n",
    "\n",
    "for loss_name in [\n",
    "    \"train_unlabeled_losses\",\n",
    "    \"test_unlabeled_losses\",\n",
    "    \"train_labeled_losses\",\n",
    "    \"test_labeled_losses\",\n",
    "]:\n",
    "    df = pd.DataFrame(fit_result[loss_name])\n",
    "    acc_name = loss_to_acc_name[loss_name]\n",
    "    df[acc_name] = lr_orth_dirs_acc_df[acc_name]\n",
    "    sns.lineplot(\n",
    "        data=df.reset_index().melt(\"index\"), x=\"index\", y=\"value\", hue=\"variable\"\n",
    "    )\n",
    "    if \"unlabeled\" in loss_name:\n",
    "        plt.axvline(best_idx, color=\"black\", linestyle=\"--\", alpha=0.3)\n",
    "    plt.suptitle(loss_name)\n",
    "    plt.xlim(0, 15)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(lr_orth_dirs_acc_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "exhaustive_ccs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
