{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import os\n",
    "import typing\n",
    "\n",
    "from extract import PrefixType\n",
    "from utils import file_utils\n",
    "from utils_extraction import load_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_ENV_VARS = dict(\n",
    "    MODEL=\"meta-llama/Llama-2-7b-chat-hf\",\n",
    "    DATASETS=\"dbpedia-14\",\n",
    "    # LABELED_DATASETS=\"imdb\",\n",
    "    EVAL_DATASETS=\"burns\",\n",
    "    PREFIX=\"normal-bananashed\",\n",
    "    METHOD_LIST=None,  # Must be set.\n",
    "    MODE=\"auto\",\n",
    "    LAYER=-1,\n",
    "    LR=1e-2,\n",
    "    N_EPOCHS=5000,\n",
    "    OPT=\"sgd\",\n",
    "    NUM_SEEDS=1,\n",
    "    N_TRIES=1,\n",
    "    SPAN_DIRS_COMBINATION=\"convex\",\n",
    "    # Saving\n",
    "    SAVE_PARAMS=False,\n",
    "    SAVE_FIT_RESULT=True,\n",
    "    SAVE_FIT_PLOTS=False,\n",
    "    SAVE_STATES=False,\n",
    "    SAVE_ORTHOGONAL_DIRECTIONS=False,\n",
    ")\n",
    "\n",
    "ALL_DATASETS = [\n",
    "    \"imdb\",\n",
    "    \"amazon-polarity\",\n",
    "    \"ag-news\",\n",
    "    \"dbpedia-14\",\n",
    "    \"copa\",\n",
    "    \"rte\",\n",
    "    \"boolq\",\n",
    "    \"qnli\",\n",
    "    \"piqa\",\n",
    "]\n",
    "ALL_DATASET_PAIRS = list(itertools.product(ALL_DATASETS, ALL_DATASETS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME_TO_TAG = {\n",
    "    \"meta-llama/Llama-2-7b-chat-hf\": \"llama-2-7b-chat-hf\",\n",
    "    \"meta-llama/Llama-2-13b-chat-hf\": \"llama-2-13b-chat-hf\",\n",
    "    \"meta-llama/Meta-Llama-3-8B\": \"meta-llama/Meta-Llama-3-8B\",\n",
    "    \"mistralai/Mistral-7B-Instruct-v0.2\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "    \"meta-llama/Meta-Llama-3-8B-Instruct\": \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "}\n",
    "\n",
    "\n",
    "def make_tag(env_vars: dict) -> str:\n",
    "    tag = \"\"\n",
    "\n",
    "    # Model\n",
    "    model = env_vars[\"MODEL\"]\n",
    "    if model not in MODEL_NAME_TO_TAG:\n",
    "        raise ValueError(f\"Unknown model: {model}\")\n",
    "    tag += MODEL_NAME_TO_TAG[model]\n",
    "\n",
    "    # Prefix\n",
    "    test_prefix = env_vars.get(\"TEST_PREFIX\", None)\n",
    "    if test_prefix is None:\n",
    "        env_vars[\"TEST_PREFIX\"] = env_vars[\"PREFIX\"]\n",
    "    elif env_vars[\"PREFIX\"] != test_prefix:\n",
    "        raise NotImplementedError(\"Different test prefix not supported\")\n",
    "    tag += f\"/{env_vars['PREFIX']}\"\n",
    "\n",
    "    # Layer\n",
    "    tag += f\"/layer_{env_vars['LAYER']}\"\n",
    "\n",
    "    # Method\n",
    "    method = env_vars[\"METHOD_LIST\"]\n",
    "    if isinstance(method, (list, tuple)):\n",
    "        if len(method) > 1:\n",
    "            raise ValueError(\"Only one method supported at a time.\")\n",
    "        method = method[0]\n",
    "    if method == \"pseudolabel\":\n",
    "        tag += \"/pseudolabel\"\n",
    "        tag += f\"/rounds_{env_vars['PSEUDOLABEL_N_ROUNDS']}\"\n",
    "        select_fn = env_vars[\"PSEUDOLABEL_SELECT_FN\"]\n",
    "        tag += f\"/select_{select_fn}\"\n",
    "        if select_fn == \"high_confidence_consistency\":\n",
    "            tag += f\"/prob_thres_{env_vars['PSEUDOLABEL_PROB_THRESHOLD']}\"\n",
    "        elif select_fn != \"all\":\n",
    "            raise NotImplementedError(f\"Unknown select_fn: {select_fn}\")\n",
    "        tag += f\"/label_{env_vars['PSEUDOLABEL_LABEL_FN']}\"\n",
    "    elif method == \"CCS+LR\":\n",
    "        mode = env_vars[\"MODE\"]\n",
    "        if mode == \"auto\":\n",
    "            raise ValueError(\"Set MODE explicitly instead of using 'auto'.\")\n",
    "\n",
    "        tag += f\"/ccs_lr/mode_{mode}/sup_weight_{env_vars['SUP_WEIGHT']}/unsup_weight_{env_vars['UNSUP_WEIGHT']}/lr_{env_vars['LR']}/n_epochs_{env_vars['N_EPOCHS']}\"\n",
    "    elif method == \"CCS+LR-in-span\":\n",
    "        mode = env_vars[\"MODE\"]\n",
    "        if mode == \"auto\":\n",
    "            raise ValueError(\"Set MODE explicitly instead of using 'auto'.\")\n",
    "\n",
    "        tag += f\"/ccs_lr_in_span/mode_{mode}/sup_weight_{env_vars['SUP_WEIGHT']}/unsup_weight_{env_vars['UNSUP_WEIGHT']}/lr_{env_vars['LR']}/n_epochs_{env_vars['N_EPOCHS']}/n_orth_dirs_{env_vars['NUM_ORTHOGONAL_DIRECTIONS']}/span_dirs_combo_{env_vars['SPAN_DIRS_COMBINATION']}\"\n",
    "    else:\n",
    "        raise NotImplementedError(f\"Method {method} not supported.\")\n",
    "\n",
    "    return tag\n",
    "\n",
    "\n",
    "def validate_env_vars(env_vars: dict) -> None:\n",
    "    prefix = env_vars[\"PREFIX\"]\n",
    "    if prefix not in typing.get_args(PrefixType):\n",
    "        raise ValueError(f\"Unknown prefix: {prefix}\")\n",
    "\n",
    "    test_prefix = env_vars.get(\"TEST_PREFIX\")\n",
    "    if test_prefix is not None and test_prefix not in typing.get_args(PrefixType):\n",
    "        raise ValueError(f\"Unknown test prefix: {test_prefix}\")\n",
    "\n",
    "    if not env_vars.get(\"METHOD_LIST\"):\n",
    "        raise ValueError(\"METHOD_LIST must be set.\")\n",
    "\n",
    "\n",
    "def make_env_vars_for_experiment(experiment_config: dict) -> dict:\n",
    "    env_vars = DEFAULT_ENV_VARS.copy()\n",
    "    env_vars.update(experiment_config)\n",
    "    env_vars[\"NAME\"] = make_tag(env_vars)\n",
    "\n",
    "    validate_env_vars(env_vars)\n",
    "    return env_vars\n",
    "\n",
    "\n",
    "MODEL_TO_SLURM_MEM = {\n",
    "    \"meta-llama/Llama-2-7b-chat-hf\": 16,\n",
    "    \"meta-llama/Llama-2-13b-chat-hf\": 16,\n",
    "}\n",
    "\n",
    "\n",
    "def make_slurm_args(env_vars: dict) -> str:\n",
    "    args = []\n",
    "    mem = MODEL_TO_SLURM_MEM.get(env_vars[\"MODEL\"])\n",
    "    if mem is not None:\n",
    "        args.append(f\"--mem={mem}gb\")\n",
    "\n",
    "    return \" \".join(args)\n",
    "\n",
    "\n",
    "def make_train_command_for_experiment(env_vars: dict, slurm: bool = True) -> str:\n",
    "    env_vars_str = \" \".join(f\"{key}={value}\" for key, value in sorted(env_vars.items()))\n",
    "    if slurm:\n",
    "        slurm_args = make_slurm_args(env_vars)\n",
    "        cmd = f\"sbatch {slurm_args} slurm_extract.sh\"\n",
    "    else:\n",
    "        cmd = \"./extract.sh\"\n",
    "\n",
    "    return f\"{env_vars_str} {cmd}\"\n",
    "\n",
    "\n",
    "def print_train_commands_for_experiments_all_datasets(\n",
    "    experiment_configs: list[dict], slurm: bool = True\n",
    "):\n",
    "    env_vars_list = []\n",
    "    for experiment_config in experiment_configs:\n",
    "        for ds in ALL_DATASETS:\n",
    "            ds_experiment_config = experiment_config.copy()\n",
    "            if \"DATASETS\" in ds_experiment_config:\n",
    "                raise ValueError(\"DATASETS should not be set in ds_experiment_config.\")\n",
    "            if \"LABELED_DATASETS\" in ds_experiment_config:\n",
    "                raise ValueError(\n",
    "                    \"LABELED_DATASETS should not be set in ds_experiment_config.\"\n",
    "                )\n",
    "            if \"EVAL_DATASETS\" in ds_experiment_config:\n",
    "                raise ValueError(\n",
    "                    \"EVAL_DATASETS should not be set in ds_experiment_config.\"\n",
    "                )\n",
    "\n",
    "            ds_experiment_config[\"DATASETS\"] = ds\n",
    "            ds_experiment_config[\"EVAL_DATASETS\"] = \"burns\"\n",
    "            env_vars_list.append(make_env_vars_for_experiment(ds_experiment_config))\n",
    "\n",
    "    for env_vars in env_vars_list:\n",
    "        print(make_train_command_for_experiment(env_vars, slurm=slurm), end=\"\\n\\n\")\n",
    "\n",
    "\n",
    "def print_train_commands_for_experiments_all_dataset_pairs(\n",
    "    experiment_configs: list[dict], slurm: bool = True\n",
    "):\n",
    "    env_vars_list = []\n",
    "    for experiment_config in experiment_configs:\n",
    "        # Iterate over all dataset pairs.\n",
    "        for labeled_ds, unlabeled_ds in ALL_DATASET_PAIRS:\n",
    "            ds_experiment_config = experiment_config.copy()\n",
    "            if \"DATASETS\" in ds_experiment_config:\n",
    "                raise ValueError(\"DATASETS should not be set in ds_experiment_config.\")\n",
    "            if \"LABELED_DATASETS\" in ds_experiment_config:\n",
    "                raise ValueError(\n",
    "                    \"LABELED_DATASETS should not be set in ds_experiment_config.\"\n",
    "                )\n",
    "            if \"EVAL_DATASETS\" in ds_experiment_config:\n",
    "                raise ValueError(\n",
    "                    \"EVAL_DATASETS should not be set in ds_experiment_config.\"\n",
    "                )\n",
    "\n",
    "            ds_experiment_config[\"DATASETS\"] = unlabeled_ds\n",
    "            ds_experiment_config[\"LABELED_DATASETS\"] = labeled_ds\n",
    "            ds_experiment_config[\"EVAL_DATASETS\"] = (\n",
    "                f'\"{list(set([labeled_ds, unlabeled_ds]))}\"'\n",
    "            )\n",
    "            env_vars_list.append(make_env_vars_for_experiment(ds_experiment_config))\n",
    "\n",
    "    for env_vars in env_vars_list:\n",
    "        print(make_train_command_for_experiment(env_vars, slurm=slurm), end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pseudo-label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_PSEUDOLABEL_ENV_VARS = dict(\n",
    "    METHOD_LIST=\"pseudolabel\",\n",
    "    MODE=\"concat\",\n",
    "    SUP_WEIGHT=1,\n",
    "    UNSUP_WEIGHT=0,\n",
    "    LR=1e-2,\n",
    "    N_EPOCHS=5000,\n",
    "    # Pseudolabel\n",
    "    PSEUDOLABEL_N_ROUNDS=5,\n",
    "    PSEUDOLABEL_SELECT_FN=\"high_confidence_consistency\",\n",
    "    PSEUDOLABEL_PROB_THRESHOLD=0.7,\n",
    "    PSEUDOLABEL_LABEL_FN=\"argmax\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## select_fn=high_confidence_consistency label_fn=argmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefixes = [\"normal\"]\n",
    "models = [\n",
    "    # \"meta-llama/Llama-2-7b-chat-hf\",\n",
    "    # \"meta-llama/Llama-2-13b-chat-hf\",\n",
    "    \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    # \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "]\n",
    "layers = [-1, -3, -5, -7, -9]\n",
    "\n",
    "# Iterate over product of parameters.\n",
    "experiment_configs = []\n",
    "for model, layer, prefix in itertools.product(models, layers, prefixes):\n",
    "    experiment_configs.append(\n",
    "        dict(DEFAULT_PSEUDOLABEL_ENV_VARS, MODEL=model, LAYER=layer, PREFIX=prefix)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment_configs for all dataset pairs\n",
    "slurm = True\n",
    "print_train_commands_for_experiments_all_dataset_pairs(experiment_configs, slurm=slurm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Only for experiment_configs\n",
    "# env_vars_list = []\n",
    "# for experiment_config in experiment_configs:\n",
    "#     env_vars_list.append(make_env_vars_for_experiment(experiment_config))\n",
    "\n",
    "# for env_vars in env_vars_list:\n",
    "#     print(make_train_command_for_experiment(env_vars), end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## select_fn=all label_fn=argmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_PSEUDOLABEL_SELECT_ALL_LABEL_ARGMAX_ENV_VARS = (\n",
    "    DEFAULT_PSEUDOLABEL_ENV_VARS.copy()\n",
    ")\n",
    "DEFAULT_PSEUDOLABEL_SELECT_ALL_LABEL_ARGMAX_ENV_VARS.update(\n",
    "    PSEUDOLABEL_N_ROUNDS=1, PSEUDOLABEL_SELECT_FN=\"all\"\n",
    ")\n",
    "\n",
    "prefixes = [\"normal\"]\n",
    "models = [\n",
    "    # \"meta-llama/Llama-2-7b-chat-hf\",\n",
    "    # \"meta-llama/Llama-2-13b-chat-hf\",\n",
    "    \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    # \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "]\n",
    "layers = [-1, -3, -5, -7, -9]\n",
    "\n",
    "# Iterate over product of parameters.\n",
    "experiment_configs = []\n",
    "for model, layer, prefix in itertools.product(models, layers, prefixes):\n",
    "    experiment_configs.append(\n",
    "        dict(\n",
    "            DEFAULT_PSEUDOLABEL_SELECT_ALL_LABEL_ARGMAX_ENV_VARS,\n",
    "            MODEL=model,\n",
    "            LAYER=layer,\n",
    "            PREFIX=prefix,\n",
    "        )\n",
    "    )\n",
    "\n",
    "print_train_commands_for_experiments_all_dataset_pairs(experiment_configs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LR (CCS+LR impl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_LR_ENV_VARS = dict(\n",
    "    METHOD_LIST=\"CCS+LR\",\n",
    "    MODE=\"concat\",\n",
    "    SUP_WEIGHT=1,\n",
    "    UNSUP_WEIGHT=0,\n",
    "    LR=1e-2,\n",
    "    N_EPOCHS=5000,\n",
    ")\n",
    "\n",
    "prefixes = [\"normal\"]\n",
    "models = [\n",
    "    # \"meta-llama/Llama-2-7b-chat-hf\",\n",
    "    # \"meta-llama/Llama-2-13b-chat-hf\",\n",
    "    \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    # \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "]\n",
    "layers = [-1, -3, -5, -7, -9]\n",
    "\n",
    "# Iterate over product of parameters.\n",
    "experiment_configs = []\n",
    "for model, layer, prefix in itertools.product(models, layers, prefixes):\n",
    "    experiment_configs.append(\n",
    "        dict(DEFAULT_LR_ENV_VARS, MODEL=model, LAYER=layer, PREFIX=prefix)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slurm = True\n",
    "\n",
    "env_vars_list = []\n",
    "for experiment_config in experiment_configs:\n",
    "    for ds in ALL_DATASETS:\n",
    "        ds_experiment_config = experiment_config.copy()\n",
    "        if \"DATASETS\" in ds_experiment_config:\n",
    "            raise ValueError(\"DATASETS should not be set in ds_experiment_config.\")\n",
    "        if \"LABELED_DATASETS\" in ds_experiment_config:\n",
    "            raise ValueError(\n",
    "                \"LABELED_DATASETS should not be set in ds_experiment_config.\"\n",
    "            )\n",
    "        if \"EVAL_DATASETS\" in ds_experiment_config:\n",
    "            raise ValueError(\"EVAL_DATASETS should not be set in ds_experiment_config.\")\n",
    "\n",
    "        ds_experiment_config[\"DATASETS\"] = ds\n",
    "        ds_experiment_config[\"LABELED_DATASETS\"] = ds\n",
    "        ds_experiment_config[\"EVAL_DATASETS\"] = \"burns\"\n",
    "        env_vars_list.append(make_env_vars_for_experiment(ds_experiment_config))\n",
    "\n",
    "for env_vars in env_vars_list:\n",
    "    print(make_train_command_for_experiment(env_vars, slurm=slurm), end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CCS (CCS+LR impl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_CCS_ENV_VARS = dict(\n",
    "    METHOD_LIST=\"CCS+LR\",\n",
    "    MODE=\"concat\",\n",
    "    SUP_WEIGHT=0,\n",
    "    UNSUP_WEIGHT=1,\n",
    "    LR=1e-2,\n",
    "    N_EPOCHS=5000,\n",
    ")\n",
    "\n",
    "prefixes = [\"normal\"]\n",
    "models = [\n",
    "    # \"meta-llama/Llama-2-7b-chat-hf\",\n",
    "    # \"meta-llama/Llama-2-13b-chat-hf\",\n",
    "    \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    # \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "]\n",
    "layers = [-1, -3, -5, -7, -9]\n",
    "\n",
    "# Iterate over product of parameters.\n",
    "experiment_configs = []\n",
    "for model, layer, prefix in itertools.product(models, layers, prefixes):\n",
    "    experiment_configs.append(\n",
    "        dict(DEFAULT_CCS_ENV_VARS, MODEL=model, LAYER=layer, PREFIX=prefix)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slurm = True\n",
    "\n",
    "env_vars_list = []\n",
    "for experiment_config in experiment_configs:\n",
    "    for ds in ALL_DATASETS:\n",
    "        ds_experiment_config = experiment_config.copy()\n",
    "        if \"DATASETS\" in ds_experiment_config:\n",
    "            raise ValueError(\"DATASETS should not be set in ds_experiment_config.\")\n",
    "        if \"LABELED_DATASETS\" in ds_experiment_config:\n",
    "            raise ValueError(\n",
    "                \"LABELED_DATASETS should not be set in ds_experiment_config.\"\n",
    "            )\n",
    "        if \"EVAL_DATASETS\" in ds_experiment_config:\n",
    "            raise ValueError(\"EVAL_DATASETS should not be set in ds_experiment_config.\")\n",
    "\n",
    "        ds_experiment_config[\"DATASETS\"] = ds\n",
    "        ds_experiment_config[\"LABELED_DATASETS\"] = ds\n",
    "        ds_experiment_config[\"EVAL_DATASETS\"] = \"burns\"\n",
    "        env_vars_list.append(make_env_vars_for_experiment(ds_experiment_config))\n",
    "\n",
    "for env_vars in env_vars_list:\n",
    "    print(make_train_command_for_experiment(env_vars, slurm=slurm), end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CCS+LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_CCS_LR_ENV_VARS = dict(\n",
    "    METHOD_LIST=\"CCS+LR\",\n",
    "    MODE=\"concat\",\n",
    "    SUP_WEIGHT=10,\n",
    "    UNSUP_WEIGHT=1,\n",
    "    LR=1e-3,\n",
    "    N_EPOCHS=10000,\n",
    ")\n",
    "\n",
    "prefixes = [\"normal\"]\n",
    "models = [\n",
    "    # \"meta-llama/Llama-2-7b-chat-hf\",\n",
    "    # \"meta-llama/Llama-2-13b-chat-hf\",\n",
    "    \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    # \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "]\n",
    "layers = [-1, -3, -5, -7, -9]\n",
    "\n",
    "# Iterate over product of parameters.\n",
    "experiment_configs = []\n",
    "for model, layer, prefix in itertools.product(models, layers, prefixes):\n",
    "    experiment_configs.append(\n",
    "        dict(DEFAULT_CCS_LR_ENV_VARS, MODEL=model, LAYER=layer, PREFIX=prefix)\n",
    "    )\n",
    "\n",
    "print_train_commands_for_experiments_all_dataset_pairs(experiment_configs, slurm=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CCS in LR span"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train orthogonal probes (LR span)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_LR_SPAN_ENV_VARS = dict(\n",
    "    METHOD_LIST=\"CCS+LR-in-span\",\n",
    "    MODE=\"concat\",\n",
    "    SUP_WEIGHT=1,\n",
    "    UNSUP_WEIGHT=0,\n",
    "    LR=1e-2,\n",
    "    N_EPOCHS=5000,\n",
    "    NUM_ORTHOGONAL_DIRECTIONS=100,\n",
    "    SPAN_DIRS_COMBINATION=\"convex\",\n",
    ")\n",
    "\n",
    "prefixes = [\"normal\"]\n",
    "models = [\n",
    "    # \"meta-llama/Llama-2-7b-chat-hf\",\n",
    "    \"meta-llama/Llama-2-13b-chat-hf\",\n",
    "    # \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "]\n",
    "layers = [-1, -3, -5, -7, -9]\n",
    "\n",
    "# Iterate over product of parameters.\n",
    "experiment_configs = []\n",
    "for model, layer, prefix in itertools.product(models, layers, prefixes):\n",
    "    experiment_configs.append(\n",
    "        dict(\n",
    "            DEFAULT_LR_SPAN_ENV_VARS,\n",
    "            MODEL=model,\n",
    "            LAYER=layer,\n",
    "            PREFIX=prefix,\n",
    "            SAVE_ORTHOGONAL_DIRECTIONS=True,\n",
    "            SAVE_FIT_PLOTS=False,\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Make a train command for each individual dataset for each experiment config.\n",
    "env_vars_list = []\n",
    "for experiment_config in experiment_configs:\n",
    "    for ds in ALL_DATASETS:\n",
    "        ds_experiment_config = experiment_config.copy()\n",
    "        if \"DATASETS\" in ds_experiment_config:\n",
    "            raise ValueError(\"DATASETS should not be set in ds_experiment_config.\")\n",
    "        if \"LABELED_DATASETS\" in ds_experiment_config:\n",
    "            raise ValueError(\n",
    "                \"LABELED_DATASETS should not be set in ds_experiment_config.\"\n",
    "            )\n",
    "        if \"EVAL_DATASETS\" in ds_experiment_config:\n",
    "            raise ValueError(\"EVAL_DATASETS should not be set in ds_experiment_config.\")\n",
    "\n",
    "        ds_experiment_config[\"DATASETS\"] = ds\n",
    "        # LABELED_DATASETS is unused by LR, so this is arbitrary.\n",
    "        ds_experiment_config[\"LABELED_DATASETS\"] = ds\n",
    "        # We don't care about the eval datasets because we're just generating\n",
    "        # the LR span, but eval on all datasets just to have the results.\n",
    "        ds_experiment_config[\"EVAL_DATASETS\"] = \"burns\"\n",
    "        env_vars_list.append(make_env_vars_for_experiment(ds_experiment_config))\n",
    "\n",
    "slurm = True\n",
    "for env_vars in env_vars_list:\n",
    "    print(make_train_command_for_experiment(env_vars, slurm=slurm), end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train CCS in LR span"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LR in CCS span"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train orthogonal probes (CCS span)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_CCS_SPAN_ENV_VARS = dict(\n",
    "    METHOD_LIST=\"CCS+LR-in-span\",\n",
    "    MODE=\"concat\",\n",
    "    SUP_WEIGHT=0,\n",
    "    UNSUP_WEIGHT=1,\n",
    "    LR=1e-2,\n",
    "    N_EPOCHS=1000,\n",
    "    NUM_ORTHOGONAL_DIRECTIONS=100,\n",
    "    SPAN_DIRS_COMBINATION=\"convex\",\n",
    ")\n",
    "\n",
    "prefixes = [\"normal\"]\n",
    "models = [\n",
    "    # \"meta-llama/Llama-2-7b-chat-hf\",\n",
    "    \"meta-llama/Llama-2-13b-chat-hf\",\n",
    "    # \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "]\n",
    "layers = [-1, -3, -5, -7, -9]\n",
    "\n",
    "# Iterate over product of parameters.\n",
    "experiment_configs = []\n",
    "for model, layer, prefix in itertools.product(models, layers, prefixes):\n",
    "    experiment_configs.append(\n",
    "        dict(\n",
    "            DEFAULT_CCS_SPAN_ENV_VARS,\n",
    "            MODEL=model,\n",
    "            LAYER=layer,\n",
    "            PREFIX=prefix,\n",
    "            SAVE_ORTHOGONAL_DIRECTIONS=True,\n",
    "            SAVE_FIT_PLOTS=False,\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Make a train command for each individual dataset for each experiment config.\n",
    "env_vars_list = []\n",
    "for experiment_config in experiment_configs:\n",
    "    for ds in ALL_DATASETS:\n",
    "        ds_experiment_config = experiment_config.copy()\n",
    "        if \"DATASETS\" in ds_experiment_config:\n",
    "            raise ValueError(\"DATASETS should not be set in ds_experiment_config.\")\n",
    "        if \"LABELED_DATASETS\" in ds_experiment_config:\n",
    "            raise ValueError(\n",
    "                \"LABELED_DATASETS should not be set in ds_experiment_config.\"\n",
    "            )\n",
    "        if \"EVAL_DATASETS\" in ds_experiment_config:\n",
    "            raise ValueError(\"EVAL_DATASETS should not be set in ds_experiment_config.\")\n",
    "\n",
    "        ds_experiment_config[\"DATASETS\"] = ds\n",
    "        # LABELED_DATASETS is unused by CCS, so this is arbitrary.\n",
    "        ds_experiment_config[\"LABELED_DATASETS\"] = ds\n",
    "        # We don't care about the eval datasets because we're just generating\n",
    "        # the LR span, but eval on all datasets just to have the results.\n",
    "        ds_experiment_config[\"EVAL_DATASETS\"] = \"burns\"\n",
    "        env_vars_list.append(make_env_vars_for_experiment(ds_experiment_config))\n",
    "\n",
    "slurm = True\n",
    "for env_vars in env_vars_list:\n",
    "    print(make_train_command_for_experiment(env_vars, slurm=slurm), end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train LR in CCS span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_LR_IN_CCS_SPAN_ENV_VARS = dict(\n",
    "    METHOD_LIST=\"CCS+LR-in-span\",\n",
    "    MODE=\"concat\",\n",
    "    SUP_WEIGHT=1,\n",
    "    UNSUP_WEIGHT=0,\n",
    "    LR=1e-2,\n",
    "    N_EPOCHS=5000,\n",
    "    SPAN_DIRS_COMBINATION=\"convex\",\n",
    ")\n",
    "\n",
    "model_to_orthogonal_ccs_directions_dir_template = {\n",
    "    \"meta-llama/Llama-2-13b-chat-hf\": \"/nas/ucb/ebronstein/Exhaustive-CCS/extraction_results/llama-2-13b-chat-hf/{prefix}/layer_{layer}/ccs_lr_in_span/mode_concat/sup_weight_0/unsup_weight_1/lr_0.01/n_epochs_1000/n_orth_dirs_100/span_dirs_combo_convex/meta-llama-Llama-2-13b-chat-hf\",\n",
    "    \"mistralai/Mistral-7B-Instruct-v0.2\": \"/nas/ucb/ebronstein/Exhaustive-CCS/extraction_results/mistralai/Mistral-7B-Instruct-v0.2/{prefix}/layer_{layer}/ccs_lr_in_span/mode_concat/sup_weight_0/unsup_weight_1/lr_0.01/n_epochs_1000/n_orth_dirs_100/span_dirs_combo_convex/mistralai-Mistral-7B-Instruct-v0.2\",\n",
    "}\n",
    "\n",
    "\n",
    "def get_orthogonal_ccs_directions_dir(\n",
    "    model: str, layer: int, prefix: str, dataset\n",
    ") -> str:\n",
    "    base_dir_template = model_to_orthogonal_ccs_directions_dir_template[model]\n",
    "    base_dir = base_dir_template.format(prefix=prefix, layer=layer)\n",
    "    # Use the same dataset for the unlabeled and labeled datasets because the\n",
    "    # CCS span generation only used the unlabeled dataset.\n",
    "    datasets_str = load_utils.get_combined_datasets_str(\n",
    "        dataset, labeled_datasets=dataset\n",
    "    )\n",
    "    load_orthogonal_directions_dir = os.path.join(base_dir, datasets_str)\n",
    "    if not os.path.exists(load_orthogonal_directions_dir):\n",
    "        raise ValueError(\n",
    "            f\"Could not find orthogonal directions directory: {load_orthogonal_directions_dir}\"\n",
    "        )\n",
    "\n",
    "    return load_orthogonal_directions_dir\n",
    "\n",
    "\n",
    "prefixes = [\"normal\"]\n",
    "models = [\n",
    "    # \"meta-llama/Llama-2-7b-chat-hf\",\n",
    "    \"meta-llama/Llama-2-13b-chat-hf\",\n",
    "    # \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "]\n",
    "layers = [-1, -5, -9]\n",
    "num_orth_dirs_list = [2, 4, 8, 16]\n",
    "\n",
    "# Iterate over product of parameters.\n",
    "experiment_configs = []\n",
    "for model, layer, prefix, num_orth_dirs in itertools.product(\n",
    "    models, layers, prefixes, num_orth_dirs_list\n",
    "):\n",
    "    for train_ds, test_ds in itertools.product(ALL_DATASETS, ALL_DATASETS):\n",
    "        # Use the orthogonal CCS directions from test_ds and train LR in\n",
    "        # their span on train_ds.\n",
    "        load_orthogonal_directions_dir = get_orthogonal_ccs_directions_dir(\n",
    "            model, layer, prefix, test_ds\n",
    "        )\n",
    "\n",
    "        experiment_config = dict(\n",
    "            DEFAULT_LR_IN_CCS_SPAN_ENV_VARS,\n",
    "            DATASETS=train_ds,  # Unused because the unsupervised weight is 0.\n",
    "            LABELED_DATASETS=train_ds,\n",
    "            EVAL_DATASETS=f'\"{list(set([train_ds, test_ds]))}\"',\n",
    "            LOAD_ORTHOGONAL_DIRECTIONS_DIR=load_orthogonal_directions_dir,\n",
    "            NUM_ORTHOGONAL_DIRECTIONS=num_orth_dirs,\n",
    "        )\n",
    "        env_vars = make_env_vars_for_experiment(experiment_config)\n",
    "        print(make_train_command_for_experiment(env_vars, slurm=True), end=\"\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
